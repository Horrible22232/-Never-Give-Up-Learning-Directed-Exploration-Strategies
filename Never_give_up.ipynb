{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Never_give_up.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXCofqoc2bVA",
        "outputId": "eddac4fc-3453-4c84-92d2-7271b9cfc286"
      },
      "source": [
        "import os\n",
        "del os.environ['LD_PRELOAD']\n",
        "\n",
        "!apt-get remove libtcmalloc*\n",
        "!apt-get update\n",
        "!apt-get install mpich build-essential qt5-default pkg-config\n",
        "!git clone https://github.com/openai/coinrun\n",
        "!pip install -r coinrun/requirements.txt\n",
        "!pip install virtualenv\n",
        "import sys\n",
        "sys.path.insert(0, 'coinrun')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'libtcmalloc-minimal4' for glob 'libtcmalloc*'\n",
            "The following packages will be REMOVED:\n",
            "  google-perftools libgoogle-perftools4 libtcmalloc-minimal4\n",
            "0 upgraded, 0 newly installed, 3 to remove and 29 not upgraded.\n",
            "After this operation, 2,187 kB disk space will be freed.\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Removing google-perftools (2.5-2.2ubuntu3) ...\n",
            "Removing libgoogle-perftools4 (2.5-2.2ubuntu3) ...\n",
            "Removing libtcmalloc-minimal4 (2.5-2.2ubuntu3) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [602 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,995 kB]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,425 kB]\n",
            "Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,746 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [333 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,398 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,165 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [363 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [893 kB]\n",
            "Fetched 12.2 MB in 6s (2,022 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu2).\n",
            "qt5-default is already the newest version (5.9.5+dfsg-0ubuntu2.5).\n",
            "The following additional packages will be installed:\n",
            "  hwloc-nox libcr-dev libcr0 libmpich-dev libmpich12\n",
            "Suggested packages:\n",
            "  blcr-dkms blcr-util mpich-doc\n",
            "The following NEW packages will be installed:\n",
            "  hwloc-nox libcr-dev libcr0 libmpich-dev libmpich12 mpich\n",
            "0 upgraded, 6 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 2,724 kB of archives.\n",
            "After this operation, 14.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libcr0 amd64 0.8.5-2.3 [18.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libcr-dev amd64 0.8.5-2.3 [24.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 hwloc-nox amd64 1.11.9-1 [157 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmpich12 amd64 3.3~a2-4 [945 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmpich-dev amd64 3.3~a2-4 [1,374 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mpich amd64 3.3~a2-4 [205 kB]\n",
            "Fetched 2,724 kB in 3s (1,024 kB/s)\n",
            "Selecting previously unselected package libcr0.\n",
            "(Reading database ... 160901 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libcr0_0.8.5-2.3_amd64.deb ...\n",
            "Unpacking libcr0 (0.8.5-2.3) ...\n",
            "Selecting previously unselected package libcr-dev.\n",
            "Preparing to unpack .../1-libcr-dev_0.8.5-2.3_amd64.deb ...\n",
            "Unpacking libcr-dev (0.8.5-2.3) ...\n",
            "Selecting previously unselected package hwloc-nox.\n",
            "Preparing to unpack .../2-hwloc-nox_1.11.9-1_amd64.deb ...\n",
            "Unpacking hwloc-nox (1.11.9-1) ...\n",
            "Selecting previously unselected package libmpich12:amd64.\n",
            "Preparing to unpack .../3-libmpich12_3.3~a2-4_amd64.deb ...\n",
            "Unpacking libmpich12:amd64 (3.3~a2-4) ...\n",
            "Selecting previously unselected package libmpich-dev.\n",
            "Preparing to unpack .../4-libmpich-dev_3.3~a2-4_amd64.deb ...\n",
            "Unpacking libmpich-dev (3.3~a2-4) ...\n",
            "Selecting previously unselected package mpich.\n",
            "Preparing to unpack .../5-mpich_3.3~a2-4_amd64.deb ...\n",
            "Unpacking mpich (3.3~a2-4) ...\n",
            "Setting up hwloc-nox (1.11.9-1) ...\n",
            "Setting up libcr0 (0.8.5-2.3) ...\n",
            "Setting up libcr-dev (0.8.5-2.3) ...\n",
            "Setting up libmpich12:amd64 (3.3~a2-4) ...\n",
            "Setting up libmpich-dev (3.3~a2-4) ...\n",
            "update-alternatives: renaming libmpi.so slave link from /usr/lib/x86_64-linux-gnu/libmpi.so to /usr/lib/libmpi.so\n",
            "update-alternatives: renaming libmpi++.so slave link from /usr/lib/x86_64-linux-gnu/libmpi++.so to /usr/lib/libmpi++.so\n",
            "Setting up mpich (3.3~a2-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Cloning into 'coinrun'...\n",
            "remote: Enumerating objects: 608, done.\u001b[K\n",
            "remote: Total 608 (delta 0), reused 0 (delta 0), pack-reused 608\u001b[K\n",
            "Receiving objects: 100% (608/608), 35.86 MiB | 23.92 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n",
            "Collecting https://github.com/openai/baselines/archive/7139a66d333b94c2dafc4af35f6a8c7598361df6.zip (from -r coinrun/requirements.txt (line 6))\n",
            "\u001b[?25l  Downloading https://github.com/openai/baselines/archive/7139a66d333b94c2dafc4af35f6a8c7598361df6.zip\n",
            "\u001b[K     \\ 5.9MB 1.4MB/s\n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.15 in /usr/local/lib/python3.7/dist-packages (from -r coinrun/requirements.txt (line 1)) (1.19.5)\n",
            "Collecting gym==0.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/2a/65b7f5abbace0eb0d607fa89902299b59277017ff70ac005b3a3707fefd2/gym-0.10.0.tar.gz (118kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 5.1MB/s \n",
            "\u001b[?25hCollecting pyglet~=1.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/1d/4a23e9aad6b309012f04e6947717237ea526b351fecd58f33fa4bda5d65c/pyglet-1.3.3-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 12.3MB/s \n",
            "\u001b[?25hCollecting mpi4py~=3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/8f/bbd8de5ba566dd77e408d8136e2bab7fdf2b97ce06cab830ba8b50a2f588/mpi4py-3.0.3.tar.gz (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 72.1MB/s \n",
            "\u001b[?25hCollecting joblib~=0.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/c9/f58220ac44a1592f79a343caba12f6837f9e0c04c196176a3d66338e1ea8/joblib-0.17.0-py3-none-any.whl (301kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 65.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.5->-r coinrun/requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.5->-r coinrun/requirements.txt (line 6)) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.5->-r coinrun/requirements.txt (line 6)) (0.3.3)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.5->-r coinrun/requirements.txt (line 6)) (3.38.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.5->-r coinrun/requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.5->-r coinrun/requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.5->-r coinrun/requirements.txt (line 6)) (4.1.2.30)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.0->-r coinrun/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.10.0->-r coinrun/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet~=1.3.2->-r coinrun/requirements.txt (line 3)) (0.16.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2->baselines==0.1.5->-r coinrun/requirements.txt (line 6)) (2.5.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.0->-r coinrun/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.0->-r coinrun/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.0->-r coinrun/requirements.txt (line 2)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.0->-r coinrun/requirements.txt (line 2)) (1.24.3)\n",
            "Building wheels for collected packages: gym, mpi4py, baselines\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.0-cp37-none-any.whl size=162337 sha256=0c260a05f13f6562e12cd93ba6a2e8bf3432719f737374cd49dc8c74edd96535\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/a1/97/43d437ba0a406667ab42abc498653356832983c69c16284290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loern7Yk2u7a"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEEzrX5r2ip9"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from baselines.common.vec_env import vec_video_recorder\n",
        "from coinrun import setup_utils, make, main_utils, wrappers\n",
        "import coinrun\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.distributions.categorical import Categorical\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from skimage.transform import resize\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "import os\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "import cv2\n",
        "from coinrun.config import Config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiszVoDDHu2y"
      },
      "source": [
        "#Render\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6e6SrLe4M3O"
      },
      "source": [
        "class RunningMeanStd(object):\n",
        "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
        "    # https://github.com/wizdom13/RND-Pytorch/blob/master/utils.py\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, 'float64')\n",
        "        self.var = np.ones(shape, 'float64')\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        delta = batch_mean - self.mean\n",
        "        tot_count = self.count + batch_count\n",
        "\n",
        "        new_mean = self.mean + delta * batch_count / tot_count\n",
        "        m_a = self.var * (self.count)\n",
        "        m_b = batch_var * (batch_count)\n",
        "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n",
        "        new_var = M2 / (self.count + batch_count)\n",
        "\n",
        "        new_count = batch_count + self.count\n",
        "\n",
        "        self.mean = new_mean\n",
        "        self.var = new_var\n",
        "        self.count = new_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsn3hwC_24_q"
      },
      "source": [
        "## Hyperparameter\n",
        "1.  TRAJECTORY_SIZE: T\n",
        "2. NUM_TRAJECTORIES: N"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gxx6vQx2o9d"
      },
      "source": [
        "PROJECT_NAME = \"NEVER GIVE UP\"\n",
        "CREATE_SAVES = False\n",
        "LOAD_SAVES = False\n",
        "\n",
        "BLACK_SCREEN_TEST = False\n",
        "EXTRINSIC_REWARD = True\n",
        "\n",
        "GAME_TYPE = \"standard\" # GAME_TYPES: standard, plattform\n",
        "HIGH_DIFFICULTY = False # Only hard and long levels = sparse reward?\n",
        "\n",
        "L = 5\n",
        "\n",
        "GAMMA = 0.99\n",
        "GAE_LAMBDA = 0.95\n",
        "TRAJECTORY_SIZE = 500\n",
        "NUM_TRAJECTORIES = 12\n",
        "LEARNING_RATE = 2.25e-4\n",
        "EMB_LEARNING_RATE = 1e-3\n",
        "EPOCHS = 5\n",
        "MINI_BATCH_SIZE = 1056\n",
        "EPSILON = 0.2\n",
        "ENTROPY_COEFF = 0.01\n",
        "MAX_GRADIENT_NORM = 0.5\n",
        "UPDATES = 10000\n",
        "SKIP_FRAMES = 1\n",
        "plot = None\n",
        "gaphics = None\n",
        "device = \"cuda\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc5iuknYapcI"
      },
      "source": [
        "if CREATE_SAVES or LOAD_SAVES:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVbHHq3pFSE8"
      },
      "source": [
        "## Plot\n",
        "Die Plotklasse ist für alle Plot zuständig. Diese Graphen werden über die Iterationen geplottet. Der jeweilige y-Wert wird mittels Methoden in der jeweiligen Liste abgespeichert. Durch die Update Funktion am Ende der Hauptschleife wird der Graph aktualisiert, angezeigt und die Iterationsvariable wird um 1 inkrementiert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5tS1tpElt0O"
      },
      "source": [
        "class Plot:\n",
        "  def __init__(self):\n",
        "    self.iter, self.iter_x = 0, [0]\n",
        "    self.running_average_reward_y, self.total_reward_y, self.episode_y, self.mean_value_y, self.level_solved_y, self.epoch_icm_loss_y, self.extrinsic_reward_y, self.black_screen_reward_y = [0], [0], [0], [0], [0], [0], [0], [0]\n",
        "    self.mean_entropy_y, self.epoch_policy_loss_y, self.epoch_value_loss_y, self.clip_frac_y, self.epoch_forward_loss_y, self.epoch_inverse_loss_y, self.intrinsic_reward_y, self.total_episodes_y = [0], [0], [0], [0], [0], [0], [0], [0]\n",
        "    self.num_episodes, self.total_episode_length, self.ratio_num, self.ratio_sum, self.total_level_solved  = 0.0, 0.0, 0.0, 0.0, 0.0\n",
        "    self.cumulate_episode = np.zeros(NUM_TRAJECTORIES)\n",
        "    self.writer = SummaryWriter()\n",
        "    self.track_tensorboard()\n",
        "\n",
        "    self.test_cumulate_episode = np.zeros(NUM_TEST_TRAJECTORIES)\n",
        "    self.test_total_episode_length, self.test_total_level_solved, self.test_num_episodes = 0, 0, 0\n",
        "    self.test_iter_x, self.test_episode_y, self.test_level_solved_y, self.test_total_episodes_y = [0], [0], [0], [0]\n",
        "\n",
        "\n",
        "  def save(self):\n",
        "    if not os.path.exists(F\"/content/drive/My Drive/CDRVL/{PROJECT_NAME}/plot\"):\n",
        "      os.makedirs(F\"/content/drive/My Drive/CDRVL/{PROJECT_NAME}/plot\")\n",
        "\n",
        "    self.save_var(\"iter_x\", self.iter_x)\n",
        "    self.save_var(\"running_average_reward_y\", self.running_average_reward_y)\n",
        "    self.save_var(\"total_reward_y\", self.total_reward_y)\n",
        "    self.save_var(\"episode_y\", self.episode_y)\n",
        "    self.save_var(\"mean_value_y\", self.mean_value_y)\n",
        "    self.save_var(\"level_solved_y\", self.level_solved_y)\n",
        "    self.save_var(\"mean_entropy_y\", self.mean_entropy_y)\n",
        "    self.save_var(\"epoch_policy_loss_y\", self.epoch_policy_loss_y)\n",
        "    self.save_var(\"epoch_value_loss_y\", self.epoch_value_loss_y)\n",
        "    self.save_var(\"clip_frac_y\", self.clip_frac_y)\n",
        "    self.save_var(\"test_iter_x\", self.test_iter_x)\n",
        "    self.save_var(\"test_episode_y\", self.test_episode_y)\n",
        "    self.save_var(\"test_level_solved_y\", self.test_level_solved_y)\n",
        "    self.save_var(\"test_total_episodes_y\", self.test_total_episodes_y)\n",
        "    self.save_var(\"total_episodes_y\", self.total_episodes_y)\n",
        "    self.save_var(\"epoch_forward_loss_y\", self.epoch_forward_loss_y)\n",
        "    self.save_var(\"epoch_inverse_loss_y\", self.epoch_inverse_loss_y)\n",
        "    self.save_var(\"epoch_icm_loss_y\", self.epoch_icm_loss_y)\n",
        "    self.save_var(\"extrinsic_reward_y\", self.extrinsic_reward_y)\n",
        "    self.save_var(\"intrinsic_reward_y\", self.intrinsic_reward_y)\n",
        "    self.save_var(\"black_screen_reward_y\", self.black_screen_reward_y)\n",
        "\n",
        "    print(\"SAVED MODEL\")\n",
        "\n",
        "  def load(self):\n",
        "    self.iter_x = self.load_var(\"iter_x\")\n",
        "    self.running_average_reward_y = self.load_var(\"running_average_reward_y\")\n",
        "    self.total_reward_y = self.load_var(\"total_reward_y\")\n",
        "    self.episode_y = self.load_var(\"episode_y\")\n",
        "    self.mean_value_y = self.load_var(\"mean_value_y\")\n",
        "    self.level_solved_y = self.load_var(\"level_solved_y\")\n",
        "    self.mean_entropy_y = self.load_var(\"mean_entropy_y\")\n",
        "    self.epoch_policy_loss_y = self.load_var(\"epoch_policy_loss_y\")\n",
        "    self.epoch_value_loss_y = self.load_var(\"epoch_value_loss_y\")\n",
        "    self.clip_frac_y = self.load_var(\"clip_frac_y\")\n",
        "    self.test_iter_x = self.load_var(\"test_iter_x\")\n",
        "    self.test_episode_y = self.load_var(\"test_episode_y\")\n",
        "    self.test_level_solved_y = self.load_var(\"test_level_solved_y\")\n",
        "    self.test_total_episodes_y = self.load_var(\"test_total_episodes_y\")\n",
        "    self.epoch_forward_loss_y = self.load_var(\"epoch_forward_loss_y\")\n",
        "    self.epoch_inverse_loss_y = self.load_var(\"epoch_inverse_loss_y\")\n",
        "    self.epoch_icm_loss_y = self.load_var(\"epoch_icm_loss_y\")\n",
        "    self.extrinsic_reward_y = self.load_var(\"extrinsic_reward_y\")\n",
        "    self.intrinsic_reward_y = self.load_var(\"intrinsic_reward_y\")\n",
        "    self.total_episodes_y = self.load_var(\"total_episodes_y\")\n",
        "    #self.black_screen_reward_y = np.zeros(len(self.intrinsic_reward_y)).tolist()\n",
        "    self.black_screen_reward_y = self.load_var(\"black_screen_reward_y\")\n",
        "    self.iter = int(self.iter_x[-1])\n",
        "    print(\"LOADED MODEL\")\n",
        "\n",
        "  def total_reward(self, total_reward):\n",
        "    self.total_reward_y.append(total_reward)\n",
        "\n",
        "  def extrinsic_reward(self, reward_e):\n",
        "    self.extrinsic_reward_y.append(reward_e)\n",
        "\n",
        "  def intrinsic_reward(self, reward_i):\n",
        "    self.intrinsic_reward_y.append(reward_i)\n",
        "\n",
        "  def mean_episodeLength(self, done):\n",
        "    self.cumulate_episode += 1 - done\n",
        "    if done.max() > 0:\n",
        "      for i in range(NUM_TRAJECTORIES):\n",
        "        if done[i]:\n",
        "          self.total_episode_length += self.cumulate_episode[i]\n",
        "          self.cumulate_episode[i] = 0\n",
        "          self.num_episodes += 1\n",
        "\n",
        "  def test_mean_episodeLength(self, done):\n",
        "    self.test_cumulate_episode += 1 - done\n",
        "    if done.max() > 0:\n",
        "      for i in range(NUM_TEST_TRAJECTORIES):\n",
        "        if done[i]:\n",
        "          self.test_total_episode_length += self.test_cumulate_episode[i]\n",
        "          self.test_cumulate_episode[i] = 0\n",
        "          self.test_num_episodes += 1\n",
        "\n",
        "  def mean_value(self, mean_value):\n",
        "      self.mean_value_y.append(mean_value)\n",
        "\n",
        "  def epoch_policy_loss(self, epoch_pol_loss):\n",
        "      self.epoch_policy_loss_y.append(epoch_pol_loss)\n",
        "\n",
        "  def epoch_value_loss(self, epoch_value_loss):\n",
        "      self.epoch_value_loss_y.append(epoch_value_loss)\n",
        "\n",
        "  def epoch_forward_loss(self, epoch_forward_loss):\n",
        "      self.epoch_forward_loss_y.append(epoch_forward_loss)\n",
        "\n",
        "  def epoch_inverse_loss(self, epoch_inverse_loss):\n",
        "      self.epoch_inverse_loss_y.append(epoch_inverse_loss)\n",
        "\n",
        "  def epoch_icm_loss(self, epoch_icm_loss):\n",
        "      self.epoch_icm_loss_y.append(epoch_icm_loss)\n",
        "\n",
        "  def mean_entropy(self, mean_entropy):\n",
        "      self.mean_entropy_y.append(mean_entropy)\n",
        "\n",
        "  def black_screen_reward(self, reward):\n",
        "      self.black_screen_reward_y.append(reward)\n",
        "\n",
        "  def clip_frac(self, ratio):\n",
        "      ratio_low = ratio < 0.8\n",
        "      ratio_up = ratio > 1.2\n",
        "      self.ratio_sum += ratio_low.sum().item() + ratio_up.sum().item()\n",
        "      self.ratio_num += ratio.shape[0]\n",
        "\n",
        "  def level_solved(self, reward_t):\n",
        "      self.total_level_solved += (reward_t > 0).sum()\n",
        "\n",
        "  def test_level_solved(self, reward_t):\n",
        "      self.test_total_level_solved += (reward_t > 0).sum()\n",
        "\n",
        "  def track_tensorboard(self):\n",
        "    self.writer.add_scalar('Reward/Total', self.total_reward_y[self.iter], self.iter)\n",
        "    self.writer.add_scalar('Reward/Running_average', self.running_average_reward_y[self.iter], self.iter)\n",
        "    self.writer.add_scalar('Episode/Length', self.episode_y[self.iter], self.iter)\n",
        "    self.writer.add_scalar('Value/Mean', self.mean_value_y[self.iter], self.iter)\n",
        "    self.writer.add_scalar('Level/Solved', self.level_solved_y[self.iter], self.iter)\n",
        "    self.writer.add_scalar('Policy/Loss/Epoch', self.epoch_policy_loss_y[self.iter], self.iter)\n",
        "    self.writer.add_scalar('Value/Loss/Epoch', self.epoch_value_loss_y[self.iter], self.iter)\n",
        "    self.writer.add_scalar('Policy/Entropy', self.mean_entropy_y[self.iter], self.iter)\n",
        "    self.writer.add_scalar('Policy/Clip_Fraction', self.clip_frac_y[self.iter], self.iter)\n",
        "\n",
        "  def test_update(self):\n",
        "    self.test_episode_y.append(self.test_total_episode_length / (self.test_num_episodes + 1e-10))\n",
        "    self.test_level_solved_y.append(self.test_total_level_solved / (self.test_num_episodes + 1e-10))\n",
        "    self.test_total_episodes_y.append(self.test_num_episodes)\n",
        "\n",
        "    self.test_total_level_solved, self.test_num_episodes, self.test_total_episode_length = 0, 0, 0.0\n",
        "    self.test_iter_x.append(self.iter + 1)\n",
        "  \n",
        "  def save_var(self, name, arr):\n",
        "    np.savetxt(F\"/content/drive/My Drive/CDRVL/{PROJECT_NAME}/plot/{name}.txt\", arr)\n",
        "\n",
        "  def load_var(self, name):\n",
        "    return np.loadtxt(F\"/content/drive/My Drive/CDRVL/{PROJECT_NAME}/plot/{name}.txt\", dtype=float).tolist()\n",
        "\n",
        "  def show(self):\n",
        "    clear_output()\n",
        "\n",
        "    self.iter += 1\n",
        "    self.iter_x.append(self.iter)\n",
        "    self.clip_frac_y.append(self.ratio_sum / self.ratio_num)\n",
        "    self.running_average_reward_y.append(sum(self.total_reward_y[-5:])/len(self.total_reward_y[-5:]))\n",
        "\n",
        "    self.episode_y.append(self.total_episode_length / (self.num_episodes + 1e-10))\n",
        "    self.total_episodes_y.append(self.num_episodes)\n",
        "    self.level_solved_y.append(self.total_level_solved / (self.num_episodes + 1e-10))\n",
        "\n",
        "    self.num_episodes, self.total_episode_length, self.ratio_num, self.ratio_sum, self.total_level_solved = 0.0, 0.0, 0.0, 0.0, 0.0\n",
        "    \n",
        "    \n",
        "    with plt.rc_context({'figure.facecolor':'white'}):\n",
        "      figure, axis = plt.subplots(3,7 ,figsize=(22,15))\n",
        "      reward_plot, episode_plot, total_reward_plot, mean_value_plot, value_loss_plot = axis[0][0], axis[0][1], axis[1][0], axis[1][1], axis[1][2]\n",
        "      policy_loss_plot, mean_entropy_plot, clip_fraction_plot, level_solved_plot = axis[0][2], axis[2][0], axis[2][1], axis[2][2]\n",
        "      forward_loss_plot, inverse_loss_plot, icm_loss_plot = axis[0][4], axis[1][4], axis[2][4] \n",
        "      extrinsic_reward_plot, intrinsic_reward_plot, black_screen_reward_plot = axis[0][5], axis[1][5], axis[2][5]\n",
        "      total_episodes_plot = axis[0][6];\n",
        "\n",
        "      test_mean_episode_length_plot, test_level_solved_plot, test_total_episodes_plot = axis[0][3], axis[1][3], axis[2][3]\n",
        "\n",
        "\n",
        "      test_mean_episode_length_plot.plot(self.test_iter_x, self.test_episode_y)\n",
        "      test_mean_episode_length_plot.set(xlabel='Iteration', ylabel='mean episode length')\n",
        "      test_mean_episode_length_plot.set_title(\"Mean-Test-Episode tracker\")\n",
        "\n",
        "      test_level_solved_plot.plot(self.test_iter_x, self.test_level_solved_y)\n",
        "      test_level_solved_plot.set(xlabel='Iteration', ylabel='% Levels Solved')\n",
        "      test_level_solved_plot.set_title(\"Test Levels finished tracker\")\n",
        "\n",
        "      total_episodes_plot.plot(self.iter_x, self.total_episodes_y)\n",
        "      total_episodes_plot.set(xlabel='Iteration', ylabel='episodes')\n",
        "      total_episodes_plot.set_title(\"Train total episodes tracker\")\n",
        "\n",
        "      test_total_episodes_plot.plot(self.test_iter_x, self.test_total_episodes_y)\n",
        "      test_total_episodes_plot.set(xlabel='Iteration', ylabel='episodes')\n",
        "      test_total_episodes_plot.set_title(\"Test-Total episodes tracker\")\n",
        "\n",
        "\n",
        "      reward_plot.plot(self.iter_x, self.running_average_reward_y)\n",
        "      reward_plot.set(xlabel='Iteration', ylabel='reward')\n",
        "      reward_plot.set_title(\"Running-Average-Reward-5 tracker\")\n",
        "      \n",
        "      episode_plot.plot(self.iter_x, self.episode_y)\n",
        "      episode_plot.set(xlabel='Iteration', ylabel='mean episode length')\n",
        "      episode_plot.set_title(\"Mean-Episode tracker\")\n",
        "\n",
        "      total_reward_plot.plot(self.iter_x, self.total_reward_y)\n",
        "      total_reward_plot.set(xlabel='Iteration', ylabel='reward')\n",
        "      total_reward_plot.set_title(\"Total-Reward tracker\")\n",
        "\n",
        "      mean_value_plot.plot(self.iter_x, self.mean_value_y)\n",
        "      mean_value_plot.set(xlabel='Iteration', ylabel='value')\n",
        "      mean_value_plot.set_title(\"Mean-Value tracker\")\n",
        "\n",
        "      value_loss_plot.plot(self.iter_x, self.epoch_value_loss_y, label = \"epoch\")\n",
        "      value_loss_plot.set(xlabel='Iteration', ylabel='L_crit')\n",
        "      value_loss_plot.set_title(\"Value-Loss tracker\")\n",
        "      value_loss_plot.legend()\n",
        "\n",
        "      policy_loss_plot.plot(self.iter_x, self.epoch_policy_loss_y, label = \"epoch\")\n",
        "      policy_loss_plot.set(xlabel='Iteration', ylabel='L_act')\n",
        "      policy_loss_plot.set_title(\"Policy-Loss tracker\")\n",
        "      policy_loss_plot.legend()\n",
        "\n",
        "      mean_entropy_plot.plot(self.iter_x, self.mean_entropy_y)\n",
        "      mean_entropy_plot.set(xlabel='Iteration', ylabel='entropy')\n",
        "      mean_entropy_plot.set_title(\"Mean-Entropy tracker\")\n",
        "\n",
        "      clip_fraction_plot.plot(self.iter_x, self.clip_frac_y)\n",
        "      clip_fraction_plot.set(xlabel='Iteration', ylabel='clip frac')\n",
        "      clip_fraction_plot.set_title(\"Clip-Fraction tracker\")\n",
        "\n",
        "      level_solved_plot.plot(self.iter_x, self.level_solved_y)\n",
        "      level_solved_plot.set(xlabel='Iteration', ylabel='% Levels Solved')\n",
        "      level_solved_plot.set_title(\"Levels finished tracker\")\n",
        "\n",
        "\n",
        "      extrinsic_reward_plot.plot(self.iter_x, self.extrinsic_reward_y)\n",
        "      extrinsic_reward_plot.set(xlabel='Iteration', ylabel='reward')\n",
        "      extrinsic_reward_plot.set_title(\"Extrinsic-Reward tracker\")\n",
        "\n",
        "      intrinsic_reward_plot.plot(self.iter_x, self.intrinsic_reward_y)\n",
        "      intrinsic_reward_plot.set(xlabel='Iteration', ylabel='reward')\n",
        "      intrinsic_reward_plot.set_title(\"Intrinsic-Reward tracker\")\n",
        "\n",
        "      self.track_tensorboard()\n",
        "      figure.show()\n",
        "      plt.pause(0.0001)  \n",
        "      time.sleep(0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6wNiYFGor5H"
      },
      "source": [
        "class Graphics:\n",
        "  def save_video(self, images, filename, rewards_i = None):\n",
        "    if rewards_i is not None:\n",
        "      images = list(map(lambda input: self.addText(*input), zip(images, rewards_i)))\n",
        "\n",
        "    video_name = filename + '.mp4'\n",
        "    height, width, layers = images[0].shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "    video = cv2.VideoWriter(video_name, fourcc, 30, (width,height))\n",
        "\n",
        "    for image in images:\n",
        "      video.write(np.array(image[...,::-1]))\n",
        "    print(\"Video saved!\")\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "    video.release()\n",
        "\n",
        "  def addText(self, image, text):\n",
        "    image = cv2.rectangle(image, (0,0), (410,32), (255,255,255), -1) \n",
        "    return cv2.putText(image,\"reward_i: \" + str(text) , (0,25), cv2.FONT_HERSHEY_SIMPLEX, 1, 255)\n",
        "\n",
        "  def show(self, screen):\n",
        "    plt.imshow(screen, aspect='auto')\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgnyMXLYGV_m"
      },
      "source": [
        "## CoinRun\n",
        "Die CoinRun Klasse ist ein Wrapper, um das Spiel Coinrun. \n",
        "In der Init Methode werden die Trajektorielänge und die Anzahl der Trajektorien bestimmt. Die Umgebung wird entsprechend dieser Variablen erstellt. \n",
        "Die step Funktion in self.envs gibt den state, reward, done, info zurück mit:\n",
        "1. state: Nx64x64x3\n",
        "2. reward: N\n",
        "3. done: N\n",
        "\n",
        "Diese werden in der Coinrun.step Methode in den jeweiligen Listen abgespeichert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIAuLBKY2rB-"
      },
      "source": [
        "class CoinRun:\n",
        "  def __init__(self, traj_length, num_traj):\n",
        "    self.num_traj = num_traj\n",
        "    self.traj_length = traj_length\n",
        "    setup_utils.setup_and_load(use_cmd_line_args=False, is_high_res = True, use_black_white=True, game_type=GAME_TYPE, high_difficulty=HIGH_DIFFICULTY)\n",
        "    self.envs = main_utils.make_general_env(num_traj)\n",
        "    self.blackScreens = None\n",
        "\n",
        "  def step(self, actor_critic, memory):\n",
        "    states, actions, rewards, masks, values, entropys, log_probs = [], [], [], [], [], [], []\n",
        "    rewards_e, rewards_i = [], []\n",
        "    video, video_meta_info = [], []\n",
        "\n",
        "    # Reset Memory\n",
        "    memory.reset()\n",
        "    #Last seen state\n",
        "    state = self.transform(self.envs.reset())\n",
        "    for _ in range(self.traj_length):\n",
        "      action, value, entropy, log_prob = self.getAction(state, actor_critic)\n",
        "      next_state, reward_e, done = self.skip_frames(action, video, video_meta_info)\n",
        "\n",
        "      reward_episodic = memory.getEpisodicReward(state.to(device), done)\n",
        "      reward_i = reward_episodic.numpy()\n",
        "\n",
        "      reward_e *= EXTRINSIC_REWARD\n",
        "      \n",
        "\n",
        "      video_meta_info = video_meta_info[:-1]\n",
        "      video_meta_info.append(round(reward_i[0].item(), 11))\n",
        "      \n",
        "      \n",
        "      #video.append(state[0].copy())\n",
        "      #graphics.show(self.envs.get_images()[0])\n",
        "\n",
        "      \n",
        "      # reward_i = np.zeros(NUM_TEST_TRAJECTORIES)\n",
        "      \n",
        "\n",
        "      states.append(state)\n",
        "      actions.append(torch.FloatTensor(action)) # needs to be float Tensor\n",
        "      rewards_e.append(reward_e)\n",
        "      rewards_i.append(reward_i)\n",
        "      rewards.append(torch.FloatTensor(reward_e) + reward_i)\n",
        "      masks.append(torch.BoolTensor(1 - done))\n",
        "      log_probs.append(log_prob)\n",
        "      values.append(value.cpu())\n",
        "      entropys.append(entropy)\n",
        "\n",
        "      state = next_state\n",
        "      \n",
        "        \n",
        "    adv_v, ref_v = self.get_adv_ref(masks, rewards, values)\n",
        "\n",
        "    next_states = torch.cat(states[1:])\n",
        "    states = torch.cat(states[:-1]).to(device)\n",
        "    actions = torch.cat(actions[:-1]).to(device)\n",
        "    rewards = torch.cat(rewards[:-1])\n",
        "    entropys = torch.cat(entropys[:-1]).detach()\n",
        "    values = torch.cat(values[:-1]).detach()\n",
        "    log_probs = torch.cat(log_probs[:-1]).detach()\n",
        "    adv_v = torch.cat(adv_v).detach()\n",
        "    ref_f = torch.cat(ref_v).detach()\n",
        "\n",
        "    plot.mean_value(values.mean())\n",
        "    plot.mean_entropy(entropys.mean())\n",
        "    plot.total_reward(rewards.sum())\n",
        "    plot.extrinsic_reward(np.sum(rewards_e[:-1]))\n",
        "    plot.intrinsic_reward(np.sum(rewards_i[:-1]))\n",
        "\n",
        "    graphics.save_video(video, \"video_iter {0}\".format(plot.iter), video_meta_info)\n",
        "\n",
        "    adv_v_normalized = (adv_v - torch.mean(adv_v)) / (torch.std(adv_v) + 1e-8)\n",
        "    \n",
        "\n",
        "    return states, actions, rewards.to(device), next_states.to(device), adv_v_normalized.to(device), ref_f.to(device), values.to(device), log_probs.to(device)\n",
        "\n",
        "\n",
        "\n",
        "  def transform(self, state):\n",
        "    # convShape = lambda s: np.array([np.moveaxis(state, 2, 0) for state in s])\n",
        "    # normalize = lambda s: [state.astype(np.float32) / 255.0 for state in s]\n",
        "    # transfrom(state) = normalize(convShape(state))\n",
        "    return torch.FloatTensor([np.moveaxis(s , 2, 0).astype(np.float32) / 255.0 for s in state])\n",
        "\n",
        "  def getAction(self, state, actor_critic):\n",
        "    distr, value = actor_critic(state.to(device))\n",
        "    action = distr.sample()\n",
        "    return action.cpu().numpy(), value.squeeze(), distr.entropy(), distr.log_prob(action)\n",
        "\n",
        "  def skip_frames(self, action, video, video_meta_info):\n",
        "    #skip frames until final state is reached and accumulate the reward\n",
        "    reward = np.zeros(NUM_TRAJECTORIES)\n",
        "    for _ in range(SKIP_FRAMES + 1):\n",
        "      state_t, reward_t, done_t, info_t = self.envs.step(action)\n",
        "      plot.mean_episodeLength(done_t)\n",
        "      plot.level_solved(reward_t)\n",
        "      video.append(self.envs.get_images()[0].copy())\n",
        "      if len(video_meta_info) > 0:\n",
        "        video_meta_info.append(video_meta_info[-1])\n",
        "      else:\n",
        "        video_meta_info.append(0)\n",
        "      reward += reward_t\n",
        "      if done_t.max() > 0:\n",
        "        break\n",
        "  \n",
        "    return self.transform(state_t), reward, done_t\n",
        "   \n",
        "\n",
        "  def get_adv_ref(self, masks, rewards, values):\n",
        "    last_gae = 0.0\n",
        "    result_adv = []\n",
        "    result_ref = []\n",
        "\n",
        "    for reward, val, mask, next_val in zip(reversed(rewards[:-1]), reversed(values[:-1]), reversed(masks[:-1]), reversed(values[1:])):\n",
        "      delta = reward + mask * GAMMA * next_val - val\n",
        "      last_gae = delta + mask * GAMMA * GAE_LAMBDA * last_gae\n",
        "      result_adv.append(last_gae)\n",
        "      result_ref.append(last_gae + val)\n",
        "    \n",
        "    adv_v = list(reversed(result_adv))\n",
        "    ref_v = list(reversed(result_ref))\n",
        "    return adv_v, ref_v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXqATuxAKOJ6"
      },
      "source": [
        "## Actor Critic\n",
        "Wichtige Anmerkung zu clipLoss: Es werden für das Plotten nur die ratios für clipfraction für die Batchoptimierung berücksichtigt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N31qfe4QCp7"
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "  def __init__(self, input_shape,  n_actions):\n",
        "    super(ActorCritic, self).__init__()\n",
        "    self.path = F\"/content/drive/My Drive/CDRVL/{PROJECT_NAME}/model/actorcritic.pt\" \n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32,64,kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,64,kernel_size=3,stride=1)\n",
        "    )\n",
        "\n",
        "    #create Input shape of pic!!!\n",
        "    conv_out_size = self._get_conv_out(input_shape)\n",
        "    self.act = nn.Sequential(\n",
        "        nn.Linear(conv_out_size, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, n_actions),\n",
        "    )\n",
        "\n",
        "    self.value = nn.Sequential(\n",
        "        nn.Linear(conv_out_size, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 1)\n",
        "    ) \n",
        "\n",
        "    # apply orthogonal init only to conv. layers \n",
        "    # my empirical tests show that the agent performs better when only self.conv is initialized and not self.act, self.value as well\n",
        "    # but the sample size was small\n",
        "    # I could be wrong\n",
        "    self.conv.apply(self.init_weights)\n",
        "\n",
        "\n",
        "  def save(self):\n",
        "    if not os.path.exists(F\"/content/drive/My Drive/CDRVL/{PROJECT_NAME}/model\"):\n",
        "      os.makedirs(F\"/content/drive/My Drive/CDRVL/{PROJECT_NAME}/model\")\n",
        "    torch.save(self.state_dict(), self.path)\n",
        "\n",
        "  def load(self):\n",
        "    self.load_state_dict(torch.load(self.path), strict=False)\n",
        "\n",
        "  def init_weights(self, m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "      nn.init.orthogonal_(m.weight, np.sqrt(2))\n",
        "\n",
        "  def forward(self, state):\n",
        "    # state.shape = (num_batches, rgb, pixel, pixel)\n",
        "    conv_out = self.conv(state).view(state.size()[0], -1)\n",
        "    # self.conv(state).shape = (n_batches, conv_output stuff)\n",
        "    # conv_out flatt it to one layer + batch size\n",
        "    # return Categorical distr. over Actions given states and Value\n",
        "    return Categorical(logits = self.act(conv_out)), self.value(conv_out)\n",
        "\n",
        "  def _get_conv_out(self, shape):\n",
        "    out = self.conv(torch.zeros(1, *shape))\n",
        "    return int(np.prod(out.size()))\n",
        "\n",
        "  def ratio_entropy(self, acts, old_act_log_p, distr):\n",
        "    new_act_log_p = distr.log_prob(acts)\n",
        "    mean_entropy = distr.entropy().mean()\n",
        "    return (new_act_log_p - old_act_log_p).exp(), mean_entropy\n",
        "\n",
        "  def entropy_clipLoss(self, acts, adv, old_act_log_p, distr):\n",
        "    r, m_entropy = self.ratio_entropy(acts, old_act_log_p, distr)\n",
        "    if(r.shape[0] <= MINI_BATCH_SIZE): #Only Plot batches\n",
        "      plot.clip_frac(r.detach())\n",
        "    r_clip = torch.clamp(r, min = 1 - EPSILON, max = 1 + EPSILON)\n",
        "    L_arr = torch.min(r * adv, r_clip * adv)\n",
        "    return -(torch.mean(L_arr) + ENTROPY_COEFF * m_entropy)\n",
        "\n",
        "  def valueLoss(self, ref, old_values, values):\n",
        "    clipped_values = old_values + (values - old_values).clamp(min=-EPSILON, max = EPSILON)\n",
        "    return 0.5 * torch.max((values - ref)**2, (clipped_values - ref)**2).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-d5AL-otuKp"
      },
      "source": [
        "# Random Network Distilation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhmbbPXGtuhm"
      },
      "source": [
        "class RND(nn.Module):\r\n",
        "  def __init__(self, input_shape,  n_actions):\r\n",
        "    super(RND, self).__init__()\r\n",
        "\r\n",
        "    self.random_Network_features = nn.Sequential(\r\n",
        "        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\r\n",
        "        nn.ReLU()\r\n",
        "    )\r\n",
        "\r\n",
        "    feature_out_size = self._get_conv_out(input_shape)\r\n",
        "    self.random_Network_output = nn.Linear(feature_out_size, 128)\r\n",
        "\r\n",
        "    self.predictor_Network_features = nn.Sequential(\r\n",
        "        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\r\n",
        "        nn.ReLU()\r\n",
        "    )\r\n",
        "\r\n",
        "    self.predictor_Network_output = nn.Linear(feature_out_size, 128)\r\n",
        "\r\n",
        "  def _get_conv_out(self, shape):\r\n",
        "    out = self.features(torch.zeros(1, *shape))\r\n",
        "    return int(np.prod(out.size()))\r\n",
        "\r\n",
        "  def forward(self, state):\r\n",
        "    f_random = self.random_Network_features(state).view(state.size()[0], -1)\r\n",
        "    f_pred = self.random_Network_features(state).view(state.size()[0], -1)\r\n",
        "\r\n",
        "    out_random = self.random_Network_output(f_random).detach()\r\n",
        "    out_pred = self.predictor_Network_output(f_pred)\r\n",
        "\r\n",
        "    return out_random, out_pred\r\n",
        "\r\n",
        "  def getLoss(self, state):\r\n",
        "    out_random, out_pred = self.forward(state)\r\n",
        "    return F.mse_loss(out_random, out_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2b-HmsU1QM3"
      },
      "source": [
        "# Embedding network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp2YBekt1Qgt"
      },
      "source": [
        "class Embedding(nn.Module):\r\n",
        "  def __init__(self, input_shape,  n_actions):\r\n",
        "    super(Embedding, self).__init__()\r\n",
        "\r\n",
        "    self.features = nn.Sequential(\r\n",
        "        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\r\n",
        "        nn.ReLU()\r\n",
        "    )\r\n",
        "\r\n",
        "    feature_out_size = self._get_conv_out(input_shape)\r\n",
        "    self.embedding = nn.Sequential(\r\n",
        "        nn.Linear(feature_out_size, 32),\r\n",
        "        nn.ReLU()\r\n",
        "    )\r\n",
        "\r\n",
        "    self.inverse = nn.Sequential(\r\n",
        "        nn.Linear(64, 128),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Linear(128, n_actions)\r\n",
        "    )\r\n",
        "\r\n",
        "    self.classes = torch.eye(n_actions)\r\n",
        "\r\n",
        "\r\n",
        "  def forward1(self, state, next_state):\r\n",
        "    f_state = self.features(state).view(state.size()[0], -1)\r\n",
        "    f_next_state = self.features(next_state).view(next_state.size()[0], -1)\r\n",
        "\r\n",
        "    emb_state = self.embedding(f_state)\r\n",
        "    emb_next_state = self.embedding(f_next_state)\r\n",
        "\r\n",
        "    return self.inverse(torch.cat((emb_state, emb_next_state), 1))\r\n",
        "\r\n",
        "  def forward(self, state):\r\n",
        "    f_state = self.features(state).view(state.size()[0], -1)\r\n",
        "    emb_state = self.embedding(f_state)\r\n",
        "    return self.embedding(f_state)\r\n",
        "\r\n",
        "  def getLoss(self, state, action, next_state):\r\n",
        "    pred_action = self.forward1(state, next_state)\r\n",
        "    enc_action = torch.FloatTensor(self.get_one_hot_encoding(action.long()))\r\n",
        " \r\n",
        "    return F.binary_cross_entropy_with_logits(pred_action, enc_action.to(device))\r\n",
        "\r\n",
        "  def _get_conv_out(self, shape):\r\n",
        "    out = self.features(torch.zeros(1, *shape))\r\n",
        "    return int(np.prod(out.size()))\r\n",
        "\r\n",
        "  def get_one_hot_encoding(self, labels):\r\n",
        "    return self.classes[labels] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7xbUXYjAN8b"
      },
      "source": [
        "# Episodic Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_6obusoAOIU"
      },
      "source": [
        "class Memory(): # Definitly not even close correct but a construct\r\n",
        "  def __init__(self, emb_network):\r\n",
        "    self.emb_states = []\r\n",
        "    self.emb_network = emb_network\r\n",
        "    self.epsilon = 10e-3\r\n",
        "    self.k = 4\r\n",
        "    self.moving_avg = np.zeros(NUM_TRAJECTORIES)\r\n",
        "    self.t = -1\r\n",
        "    self.currIndex = np.zeros(NUM_TRAJECTORIES)\r\n",
        "    self.runningIndex = np.zeros(NUM_TRAJECTORIES)\r\n",
        "\r\n",
        "  def getEpisodicReward(self, state, done):\r\n",
        "    epis_reward = np.zeros(NUM_TRAJECTORIES)\r\n",
        "    self.t = -1\r\n",
        "    \r\n",
        "    \r\n",
        "    self.currIndex = (1 - done) * self.currIndex + done * self.runningIndex\r\n",
        "    self.runningIndex += 1\r\n",
        "\r\n",
        "    emb_state = self.emb_network(state).detach().cpu().numpy()\r\n",
        "    if len(self.emb_states) > self.k:\r\n",
        "      nn = self.knn(emb_state)\r\n",
        "      epis_reward = torch.FloatTensor(self.calculateReward(emb_state, nn))\r\n",
        "      \r\n",
        "\r\n",
        "    self.emb_states.append(emb_state)\r\n",
        "    return torch.FloatTensor(epis_reward)\r\n",
        "\r\n",
        "\r\n",
        "  def calculateReward(self, emb_state, nn):\r\n",
        "    rewards = []\r\n",
        "    for i in range(NUM_TRAJECTORIES):\r\n",
        "      rewards.append(1 / (np.sqrt(np.sum(list(map(lambda onn: self.kernel(emb_state, onn), nn[i])))) + 1e-3))\r\n",
        "    return rewards\r\n",
        "\r\n",
        "  def reset(self):\r\n",
        "    self.emb_states = []\r\n",
        "    self.t = -1\r\n",
        "    self.moving_avg = np.zeros(NUM_TRAJECTORIES)\r\n",
        "    self.currIndex = np.zeros(NUM_TRAJECTORIES)\r\n",
        "    self.runningIndex = np.zeros(NUM_TRAJECTORIES)\r\n",
        "\r\n",
        "  def kernel(self, x, y):\r\n",
        "    return self.epsilon / ((self.d(x, y)**2 / (self.d2m(x, y)**2 + self.epsilon))  + self.epsilon)\r\n",
        "\r\n",
        "  def d(self, x, y):\r\n",
        "    return np.linalg.norm(x - y)\r\n",
        "  \r\n",
        "  def d2m(self, x, y):\r\n",
        "    self.t += 1\r\n",
        "    index = int(self.t / self.k)\r\n",
        "    self.moving_avg[index] = self.moving_avg[index] + 1. / self.k * (self.d(x, y) - self.moving_avg[index])\r\n",
        "    return self.moving_avg[index]\r\n",
        "\r\n",
        "  def calcKNN(self, emb_state, x):\r\n",
        "    if len(x) < self.k:\r\n",
        "      return [emb_state for _ in range(self.k)]\r\n",
        "\r\n",
        "    knn_index = np.array(list(map(lambda y: self.d(emb_state, y), x))).argsort()[:self.k]\r\n",
        "    return [x[i] for i in knn_index]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  def knn(self, emb_state):\r\n",
        "    getElements = lambda index: [self.emb_states[elem][index] for elem in range(int(self.currIndex[index]), len(self.emb_states))]\r\n",
        "    array = np.array([getElements(i) for i in range(NUM_TRAJECTORIES)])\r\n",
        "\r\n",
        "    sol = np.array(list(map(lambda x: self.calcKNN(emb_state, x), array)))\r\n",
        "    return sol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrC1E7AYtvvb"
      },
      "source": [
        "# Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF0fefWyQ0V5"
      },
      "source": [
        "plot, graphics = Plot(), Graphics()\n",
        "coinRun = CoinRun(TRAJECTORY_SIZE, NUM_TRAJECTORIES)\n",
        "test_env = TestEnv(TEST_ITERS, NUM_TEST_TRAJECTORIES)\n",
        "input_shape = coinRun.envs.observation_space.shape\n",
        "input_shape = (input_shape[2], input_shape[1], input_shape[0])\n",
        "\n",
        "actor_critic = ActorCritic(input_shape , coinRun.envs.action_space.n).to(device)\n",
        "emb_network = Embedding(input_shape, coinRun.envs.action_space.n).to(device)\n",
        "\n",
        "CONST_LEARNING_RATE, CONST_EPSILON = LEARNING_RATE, EPSILON\n",
        "\n",
        "emb_optimizer =  optim.Adam(emb_network.parameters(), lr = EMB_LEARNING_RATE)\n",
        "\n",
        "memory = Memory(emb_network)\n",
        "generation = 0\n",
        "if LOAD_SAVES:\n",
        "  plot.load()\n",
        "  actor_critic.load()\n",
        "  generation = plot.iter\n",
        "\n",
        "for update in range(generation, UPDATES):\n",
        "  print(\"generation: \", generation)\n",
        "  generation += 1\n",
        "\n",
        "  progress = update / UPDATES\n",
        "  LEARNING_RATE = CONST_LEARNING_RATE * (1-progress)\n",
        "  EPSILON = CONST_EPSILON * (1-progress)\n",
        "  actor_critic_optimizer = optim.Adam(actor_critic.parameters(), lr = LEARNING_RATE)\n",
        "  if (generation - 1) % 50 == 0 and not LOAD_SAVES: # Since we save here, we don't need to evaluate the testenv twice\n",
        "    if CREATE_SAVES and generation > 1:\n",
        "      plot.save()\n",
        "      actor_critic.save()\n",
        "\n",
        "  states, actions, rewards, next_states, advs, refs, values, old_act_log_probs  = coinRun.step(actor_critic, memory)\n",
        "\n",
        "\n",
        "  epoch_actor_loss, epoch_critic_loss, epoch_forward_loss, epoch_inverse_loss = 0, 0, 0, 0\n",
        "  for epoch in range(EPOCHS):\n",
        "    permutation = torch.randperm(states.shape[0])\n",
        "    for i in range(0, states.shape[0], MINI_BATCH_SIZE):\n",
        "      indices = permutation[i:i+MINI_BATCH_SIZE]\n",
        "      batch_states, batch_acts, batch_rews, batch_next_states, batch_advs, batch_refs, batch_old_act_log_p, batch_values = states[indices], actions[indices], rewards[indices], next_states[indices], advs[indices], refs[indices], old_act_log_probs[indices], values[indices]\n",
        "\n",
        "      actor_critic.zero_grad()\n",
        "      distr, new_values = actor_critic(batch_states)\n",
        "      actor_loss = actor_critic.entropy_clipLoss(batch_acts, batch_advs, batch_old_act_log_p, distr)\n",
        "      critic_loss = 0.5 * actor_critic.valueLoss(batch_refs, batch_values, new_values.squeeze(-1))\n",
        "      loss = actor_loss + critic_loss\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(actor_critic.parameters(), MAX_GRADIENT_NORM)\n",
        "      actor_critic_optimizer.step()\n",
        "\n",
        "      emb_network.zero_grad()\n",
        "      emb_loss = emb_network.getLoss(batch_states, batch_acts, batch_next_states)\n",
        "      emb_loss.backward()\n",
        "      emb_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "  plot.epoch_policy_loss(epoch_actor_loss / (epoch * states.shape[0] / MINI_BATCH_SIZE))\n",
        "  plot.epoch_value_loss(epoch_critic_loss / (epoch * states.shape[0] / MINI_BATCH_SIZE))\n",
        "\n",
        "\n",
        "  plot.show()\n",
        "\n",
        "  LOAD_SAVES = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjIVgM1AbzpH"
      },
      "source": [
        "[1,2] * 2     \r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}