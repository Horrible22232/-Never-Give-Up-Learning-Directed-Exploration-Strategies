{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "PPO_RND_Aufgaben.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ytOBAt6eCEy"
      },
      "source": [
        "# Aufgaben\n",
        "\n",
        "Die Stellen, an denen Aufgaben gelöst werden sollen, sind im Code mit Kommentaren versehen.\n",
        "\n",
        "1) Definiert einen passenden Dense Layer im RandomTargetNetwork.\n",
        "\n",
        "2) Definiert eine mögliche Berechnung der intrinsischen Belohnung. \n",
        "\n",
        "3) Normalisiere die intrinsische Belohnung, sodass die Skallierung dieser unabhängig von der aktuellen Umwelt ist. Folgende Quelle könnte dabei hilfreich sein: [RND Paper](https://arxiv.org/pdf/1810.12894.pdf) (S.5)\n",
        "\n",
        "4) Das Training kann jederzeit unterbrochen werden. Brecht das Training ab, sobald der Agent eine extrinsische Belohnung von mind. .... hat und visualisiert das Ergebnis als gif."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxxu8H-N9Mcm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fp2GnwWT7--",
        "outputId": "5fd4786b-060e-4933-f481-14359993c390"
      },
      "source": [
        "!git clone https://github.com/openai/baselines.git\n",
        "!apt-get install imagemagick"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'baselines'...\n",
            "remote: Enumerating objects: 3627, done.\u001b[K\n",
            "remote: Total 3627 (delta 0), reused 0 (delta 0), pack-reused 3627\u001b[K\n",
            "Receiving objects: 100% (3627/3627), 6.46 MiB | 7.29 MiB/s, done.\n",
            "Resolving deltas: 100% (2429/2429), done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono ghostscript gsfonts\n",
            "  imagemagick-6-common imagemagick-6.q16 libcupsfilters1 libcupsimage2\n",
            "  libdjvulibre-text libdjvulibre21 libgs9 libgs9-common libijs-0.35\n",
            "  libjbig2dec0 liblqr-1-0 libmagickcore-6.q16-3 libmagickcore-6.q16-3-extra\n",
            "  libmagickwand-6.q16-3 libnetpbm10 libwmf0.2-7 netpbm poppler-data\n",
            "Suggested packages:\n",
            "  fonts-noto ghostscript-x imagemagick-doc autotrace cups-bsd | lpr | lprng\n",
            "  enscript gimp gnuplot grads hp2xx html2ps libwmf-bin mplayer povray radiance\n",
            "  sane-utils texlive-base-bin transfig ufraw-batch inkscape libjxr-tools\n",
            "  libwmf0.2-7-gtk poppler-utils fonts-japanese-mincho | fonts-ipafont-mincho\n",
            "  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
            "  fonts-arphic-uming fonts-nanum\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono ghostscript gsfonts imagemagick\n",
            "  imagemagick-6-common imagemagick-6.q16 libcupsfilters1 libcupsimage2\n",
            "  libdjvulibre-text libdjvulibre21 libgs9 libgs9-common libijs-0.35\n",
            "  libjbig2dec0 liblqr-1-0 libmagickcore-6.q16-3 libmagickcore-6.q16-3-extra\n",
            "  libmagickwand-6.q16-3 libnetpbm10 libwmf0.2-7 netpbm poppler-data\n",
            "0 upgraded, 23 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 18.4 MB of archives.\n",
            "After this operation, 66.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblqr-1-0 amd64 0.4.2-2.1 [27.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 imagemagick-6-common all 8:6.9.7.4+dfsg-16ubuntu6.9 [60.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagickcore-6.q16-3 amd64 8:6.9.7.4+dfsg-16ubuntu6.9 [1,616 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagickwand-6.q16-3 amd64 8:6.9.7.4+dfsg-16ubuntu6.9 [293 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 poppler-data all 0.4.8-2 [1,479 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-noto-mono all 20171026-2 [75.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsimage2 amd64 2.2.7-1ubuntu2.8 [18.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libijs-0.35 amd64 0.35-13 [15.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig2dec0 amd64 0.13-6 [55.9 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9-common all 9.26~dfsg+0-0ubuntu0.18.04.14 [5,092 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9 amd64 9.26~dfsg+0-0ubuntu0.18.04.14 [2,265 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ghostscript amd64 9.26~dfsg+0-0ubuntu0.18.04.14 [51.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.4 [3,120 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 imagemagick-6.q16 amd64 8:6.9.7.4+dfsg-16ubuntu6.9 [423 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 imagemagick amd64 8:6.9.7.4+dfsg-16ubuntu6.9 [14.2 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsfilters1 amd64 1.20.2-0ubuntu3.1 [108 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdjvulibre-text all 3.5.27.1-8ubuntu0.2 [49.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdjvulibre21 amd64 3.5.27.1-8ubuntu0.2 [560 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwmf0.2-7 amd64 0.2.8.4-12 [150 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagickcore-6.q16-3-extra amd64 8:6.9.7.4+dfsg-16ubuntu6.9 [62.3 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnetpbm10 amd64 2:10.0-15.3build1 [58.0 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 netpbm amd64 2:10.0-15.3build1 [1,017 kB]\n",
            "Fetched 18.4 MB in 4s (4,666 kB/s)\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 160690 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package liblqr-1-0:amd64.\n",
            "Preparing to unpack .../01-liblqr-1-0_0.4.2-2.1_amd64.deb ...\n",
            "Unpacking liblqr-1-0:amd64 (0.4.2-2.1) ...\n",
            "Selecting previously unselected package imagemagick-6-common.\n",
            "Preparing to unpack .../02-imagemagick-6-common_8%3a6.9.7.4+dfsg-16ubuntu6.9_all.deb ...\n",
            "Unpacking imagemagick-6-common (8:6.9.7.4+dfsg-16ubuntu6.9) ...\n",
            "Selecting previously unselected package libmagickcore-6.q16-3:amd64.\n",
            "Preparing to unpack .../03-libmagickcore-6.q16-3_8%3a6.9.7.4+dfsg-16ubuntu6.9_amd64.deb ...\n",
            "Unpacking libmagickcore-6.q16-3:amd64 (8:6.9.7.4+dfsg-16ubuntu6.9) ...\n",
            "Selecting previously unselected package libmagickwand-6.q16-3:amd64.\n",
            "Preparing to unpack .../04-libmagickwand-6.q16-3_8%3a6.9.7.4+dfsg-16ubuntu6.9_amd64.deb ...\n",
            "Unpacking libmagickwand-6.q16-3:amd64 (8:6.9.7.4+dfsg-16ubuntu6.9) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../05-poppler-data_0.4.8-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.8-2) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../06-fonts-noto-mono_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-mono (20171026-2) ...\n",
            "Selecting previously unselected package libcupsimage2:amd64.\n",
            "Preparing to unpack .../07-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n",
            "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../08-libijs-0.35_0.35-13_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../09-libjbig2dec0_0.13-6_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../10-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.14_all.deb ...\n",
            "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../11-libgs9_9.26~dfsg+0-0ubuntu0.18.04.14_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../12-ghostscript_9.26~dfsg+0-0ubuntu0.18.04.14_amd64.deb ...\n",
            "Unpacking ghostscript (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package gsfonts.\n",
            "Preparing to unpack .../13-gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.4_all.deb ...\n",
            "Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Selecting previously unselected package imagemagick-6.q16.\n",
            "Preparing to unpack .../14-imagemagick-6.q16_8%3a6.9.7.4+dfsg-16ubuntu6.9_amd64.deb ...\n",
            "Unpacking imagemagick-6.q16 (8:6.9.7.4+dfsg-16ubuntu6.9) ...\n",
            "Selecting previously unselected package imagemagick.\n",
            "Preparing to unpack .../15-imagemagick_8%3a6.9.7.4+dfsg-16ubuntu6.9_amd64.deb ...\n",
            "Unpacking imagemagick (8:6.9.7.4+dfsg-16ubuntu6.9) ...\n",
            "Selecting previously unselected package libcupsfilters1:amd64.\n",
            "Preparing to unpack .../16-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libdjvulibre-text.\n",
            "Preparing to unpack .../17-libdjvulibre-text_3.5.27.1-8ubuntu0.2_all.deb ...\n",
            "Unpacking libdjvulibre-text (3.5.27.1-8ubuntu0.2) ...\n",
            "Selecting previously unselected package libdjvulibre21:amd64.\n",
            "Preparing to unpack .../18-libdjvulibre21_3.5.27.1-8ubuntu0.2_amd64.deb ...\n",
            "Unpacking libdjvulibre21:amd64 (3.5.27.1-8ubuntu0.2) ...\n",
            "Selecting previously unselected package libwmf0.2-7:amd64.\n",
            "Preparing to unpack .../19-libwmf0.2-7_0.2.8.4-12_amd64.deb ...\n",
            "Unpacking libwmf0.2-7:amd64 (0.2.8.4-12) ...\n",
            "Selecting previously unselected package libmagickcore-6.q16-3-extra:amd64.\n",
            "Preparing to unpack .../20-libmagickcore-6.q16-3-extra_8%3a6.9.7.4+dfsg-16ubuntu6.9_amd64.deb ...\n",
            "Unpacking libmagickcore-6.q16-3-extra:amd64 (8:6.9.7.4+dfsg-16ubuntu6.9) ...\n",
            "Selecting previously unselected package libnetpbm10.\n",
            "Preparing to unpack .../21-libnetpbm10_2%3a10.0-15.3build1_amd64.deb ...\n",
            "Unpacking libnetpbm10 (2:10.0-15.3build1) ...\n",
            "Selecting previously unselected package netpbm.\n",
            "Preparing to unpack .../22-netpbm_2%3a10.0-15.3build1_amd64.deb ...\n",
            "Unpacking netpbm (2:10.0-15.3build1) ...\n",
            "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up imagemagick-6-common (8:6.9.7.4+dfsg-16ubuntu6.9) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Setting up poppler-data (0.4.8-2) ...\n",
            "Setting up libdjvulibre-text (3.5.27.1-8ubuntu0.2) ...\n",
            "Setting up libnetpbm10 (2:10.0-15.3build1) ...\n",
            "Setting up fonts-noto-mono (20171026-2) ...\n",
            "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Setting up liblqr-1-0:amd64 (0.4.2-2.1) ...\n",
            "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
            "Setting up netpbm (2:10.0-15.3build1) ...\n",
            "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up libwmf0.2-7:amd64 (0.2.8.4-12) ...\n",
            "Setting up libmagickcore-6.q16-3:amd64 (8:6.9.7.4+dfsg-16ubuntu6.9) ...\n",
            "Setting up libdjvulibre21:amd64 (3.5.27.1-8ubuntu0.2) ...\n",
            "Setting up ghostscript (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up libmagickwand-6.q16-3:amd64 (8:6.9.7.4+dfsg-16ubuntu6.9) ...\n",
            "Setting up imagemagick-6.q16 (8:6.9.7.4+dfsg-16ubuntu6.9) ...\n",
            "update-alternatives: using /usr/bin/compare-im6.q16 to provide /usr/bin/compare (compare) in auto mode\n",
            "update-alternatives: using /usr/bin/compare-im6.q16 to provide /usr/bin/compare-im6 (compare-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/animate-im6.q16 to provide /usr/bin/animate (animate) in auto mode\n",
            "update-alternatives: using /usr/bin/animate-im6.q16 to provide /usr/bin/animate-im6 (animate-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/convert-im6.q16 to provide /usr/bin/convert (convert) in auto mode\n",
            "update-alternatives: using /usr/bin/convert-im6.q16 to provide /usr/bin/convert-im6 (convert-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/composite-im6.q16 to provide /usr/bin/composite (composite) in auto mode\n",
            "update-alternatives: using /usr/bin/composite-im6.q16 to provide /usr/bin/composite-im6 (composite-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/conjure-im6.q16 to provide /usr/bin/conjure (conjure) in auto mode\n",
            "update-alternatives: using /usr/bin/conjure-im6.q16 to provide /usr/bin/conjure-im6 (conjure-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/import-im6.q16 to provide /usr/bin/import (import) in auto mode\n",
            "update-alternatives: using /usr/bin/import-im6.q16 to provide /usr/bin/import-im6 (import-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/identify-im6.q16 to provide /usr/bin/identify (identify) in auto mode\n",
            "update-alternatives: using /usr/bin/identify-im6.q16 to provide /usr/bin/identify-im6 (identify-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/stream-im6.q16 to provide /usr/bin/stream (stream) in auto mode\n",
            "update-alternatives: using /usr/bin/stream-im6.q16 to provide /usr/bin/stream-im6 (stream-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/display-im6.q16 to provide /usr/bin/display (display) in auto mode\n",
            "update-alternatives: using /usr/bin/display-im6.q16 to provide /usr/bin/display-im6 (display-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/montage-im6.q16 to provide /usr/bin/montage (montage) in auto mode\n",
            "update-alternatives: using /usr/bin/montage-im6.q16 to provide /usr/bin/montage-im6 (montage-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/mogrify-im6.q16 to provide /usr/bin/mogrify (mogrify) in auto mode\n",
            "update-alternatives: using /usr/bin/mogrify-im6.q16 to provide /usr/bin/mogrify-im6 (mogrify-im6) in auto mode\n",
            "Setting up libmagickcore-6.q16-3-extra:amd64 (8:6.9.7.4+dfsg-16ubuntu6.9) ...\n",
            "Setting up imagemagick (8:6.9.7.4+dfsg-16ubuntu6.9) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ph1J_CP9Mcu"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, 'baselines')\n",
        "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUqlvW1bfM73"
      },
      "source": [
        "## Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU1u94nASTPF"
      },
      "source": [
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done  = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
        "            # so it's important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip       = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n",
        "        \"\"\"\n",
        "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n",
        "        observation should be warped.\n",
        "        \"\"\"\n",
        "        super().__init__(env)\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grayscale = grayscale\n",
        "        self._key = dict_space_key\n",
        "        if self._grayscale:\n",
        "            num_colors = 1\n",
        "        else:\n",
        "            num_colors = 3\n",
        "\n",
        "        new_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=(self._height, self._width, num_colors),\n",
        "            dtype=np.uint8,\n",
        "        )\n",
        "        if self._key is None:\n",
        "            original_space = self.observation_space\n",
        "            self.observation_space = new_space\n",
        "        else:\n",
        "            original_space = self.observation_space.spaces[self._key]\n",
        "            self.observation_space.spaces[self._key] = new_space\n",
        "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
        "\n",
        "    def observation(self, obs):\n",
        "        if self._key is None:\n",
        "            frame = obs\n",
        "        else:\n",
        "            frame = obs[self._key]\n",
        "\n",
        "        if self._grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(\n",
        "            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "        if self._grayscale:\n",
        "            frame = np.expand_dims(frame, -1)\n",
        "\n",
        "        if self._key is None:\n",
        "            obs = frame\n",
        "        else:\n",
        "            obs = obs.copy()\n",
        "            obs[self._key] = frame\n",
        "        return obs\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Change image shape to CWH\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.swapaxes(observation, 2, 0)\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "        self._out = None\n",
        "\n",
        "    def _force(self):\n",
        "        if self._out is None:\n",
        "            self._out = np.concatenate(self._frames, axis=-1)\n",
        "            self._frames = None\n",
        "        return self._out\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = self._force()\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._force())\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._force()[i]\n",
        "\n",
        "    def count(self):\n",
        "        frames = self._force()\n",
        "        return frames.shape[frames.ndim - 1]\n",
        "\n",
        "    def frame(self, i):\n",
        "        return self._force()[..., i]\n",
        "\n",
        "def make_atari(env_id, max_episode_steps=None):\n",
        "    env = gym.make(env_id)\n",
        "    assert 'NoFrameskip' in env.spec.id\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    if max_episode_steps is not None:\n",
        "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
        "    return env\n",
        "\n",
        "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=True, scale=False):\n",
        "    \"\"\"Configure environment for DeepMind-style Atari.\n",
        "    \"\"\"\n",
        "    if episode_life:\n",
        "        env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    if scale:\n",
        "        env = ScaledFloatFrame(env)\n",
        "    if clip_rewards:\n",
        "        env = ClipRewardEnv(env)\n",
        "    if frame_stack:\n",
        "        env = FrameStack(env, 4)\n",
        "    return ImageToPyTorch(env)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYoTfrgh9Mcw"
      },
      "source": [
        "## Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7_ZB3xcC9Mcw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69ed273-0625-43bd-b035-0ce0b2f7d1cf"
      },
      "source": [
        "'PPO Settings'''\n",
        "TRAJ_LEN = 128\n",
        "N_OPT_EPOCHS = 10\n",
        "ENT_COEF = 1e-2\n",
        "CLIP_RANGE = 0.1\n",
        "LAMBDA = 0.95\n",
        "\n",
        "'''RND Settings'''\n",
        "# RND start step for input normalization\n",
        "RND_START = int(0)\n",
        "# Discount rate for intrinsic reward\n",
        "INT_GAMMA = 0.99\n",
        "\n",
        "'''Environment Settings'''\n",
        "# sequential images to define state\n",
        "STATE_LEN = 4\n",
        "# openai gym env name\n",
        "ENV_NAME = 'BreakoutNoFrameskip-v4'\n",
        "# number of environments for A2C\n",
        "N_ENVS = 4\n",
        "# define gym \n",
        "env = DummyVecEnv([lambda: wrap_deepmind(make_atari(ENV_NAME)) for i in range(N_ENVS)])\n",
        "\n",
        "# check gym setting\n",
        "N_ACTIONS = env.action_space.n;print('N_ACTIONS : ', N_ACTIONS) #  6\n",
        "N_STATES = env.observation_space.shape;print('N_STATES : ', N_STATES) # (4, 84, 84)\n",
        "# Total simulation step\n",
        "N_STEP = int(1e+7)\n",
        "# gamma for MDP\n",
        "GAMMA = 0.9999\n",
        "# visualize for agent playing\n",
        "RENDERING = False\n",
        "\n",
        "'''Training settings'''\n",
        "# check GPU usage\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "print('USE GPU: '+str(USE_GPU))\n",
        "# mini-batch size\n",
        "BATCH_SIZE = 128\n",
        "# learning rage\n",
        "LR = 1e-4\n",
        "# clip gradient\n",
        "MAX_GRAD_NORM = 0.1\n",
        "# log optimization\n",
        "LOG_OPT = False\n",
        "\n",
        "'''Save&Load Settings'''\n",
        "# log frequency\n",
        "LOG_FREQ = 10\n",
        "# check save/load\n",
        "SAVE = False\n",
        "LOAD = False\n",
        "# paths for predction net, target net, result log\n",
        "NET_PATH = './data/model/ppo_net.pkl'\n",
        "PRED_PATH = './data/model/pred_net.pkl'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N_ACTIONS :  4\n",
            "N_STATES :  (4, 84, 84)\n",
            "USE GPU: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcGeVltC9Mcy"
      },
      "source": [
        "## Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFX_bEIT9Mcy"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # architecture def\n",
        "        self.feature_extraction = nn.Sequential(\n",
        "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        # actor\n",
        "        self.actor = nn.Linear(256, N_ACTIONS)\n",
        "        # extrinsic critic\n",
        "        self.critic = nn.Linear(256, 1)\n",
        "        # intrinsic critic\n",
        "        self.int_critic = nn.Linear(256, 1)\n",
        "            \n",
        "        # parameter initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a tensor of (m, 4, 84, 84)\n",
        "        x = self.feature_extraction(x / 255.0)\n",
        "        # x.size(0) : mini-batch size\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(F.relu(x))\n",
        "        x = self.fc2(F.relu(x))\n",
        "        # use log_softmax for numerical stability\n",
        "        action_log_prob = F.log_softmax(self.actor(F.relu(x)), dim=1)\n",
        "        state_value = self.critic(F.relu(x))\n",
        "        int_state_value = self.int_critic(F.relu(x))\n",
        "\n",
        "        return action_log_prob, state_value, int_state_value\n",
        "\n",
        "    def save(self, PATH):\n",
        "        torch.save(self.state_dict(),PATH)\n",
        "\n",
        "    def load(self, PATH):\n",
        "        self.load_state_dict(torch.load(PATH))\n",
        "        \n",
        "class RandomPredNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RandomPredNet, self).__init__()\n",
        "        # architecture def\n",
        "        self.feature_extraction = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
        "            nn.LeakyReLU(negative_slope=2e-1),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.LeakyReLU(negative_slope=2e-1),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 256)\n",
        "        # one more layer than target network for enough capacity\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            \n",
        "\n",
        "    def forward(self, x):\n",
        "        # if you use feature normalization in RND, remove division by 255.0\n",
        "        x = self.feature_extraction(x / 255.0)\n",
        "        # x.size(0) : mini-batch size\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(F.leaky_relu(x, negative_slope=2e-1))\n",
        "        x = self.fc2(F.leaky_relu(x, negative_slope=2e-1))\n",
        "        return x\n",
        "    \n",
        "    def save(self, PATH):\n",
        "        torch.save(self.state_dict(),PATH)\n",
        "\n",
        "    def load(self, PATH):\n",
        "        self.load_state_dict(torch.load(PATH))\n",
        "        \n",
        "class RandomTargetNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RandomTargetNet, self).__init__()\n",
        "        # architecture def\n",
        "        self.feature_extraction = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "        )\n",
        "\n",
        "        # Aufgabe 1\n",
        "        self.fc1 = lambda x: torch.zeros((x.size(0), 256)).to(\"cuda\")\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # if you use feature normalization in RND, remove division by 255.0\n",
        "        x = self.feature_extraction(x / 255.0)\n",
        "        # x.size(0) : mini-batch size\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(F.relu(x))\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVUvCmGa9Mc3"
      },
      "source": [
        "## RND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY292hJZ9Mc5"
      },
      "source": [
        "class PPO:\n",
        "    def __init__(self):\n",
        "        self.net = ConvNet()\n",
        "        self.rand_target = RandomTargetNet()\n",
        "        self.rand_pred = RandomPredNet()\n",
        "        # use gpu\n",
        "        if USE_GPU:\n",
        "            self.net = self.net.cuda()\n",
        "            self.rand_target = self.rand_target.cuda()\n",
        "            self.rand_pred = self.rand_pred.cuda()\n",
        "            \n",
        "        # simulator step conter\n",
        "        self.memory_counter = 0\n",
        "        \n",
        "        # define optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=LR)\n",
        "        # define optimizer for predict network\n",
        "        self.rand_pred_opt = torch.optim.Adam(self.rand_pred.parameters(), lr=LR)\n",
        "        \n",
        "        # ppo clip range\n",
        "        self.clip_range = CLIP_RANGE\n",
        "        \n",
        "        # observation statistics for RND (if you use feature normalization in RND)\n",
        "        self.s_mu = None\n",
        "        self.s_sigma = None\n",
        "        \n",
        "    def save_model(self):\n",
        "        self.net.cpu()\n",
        "        self.rand_pred.cpu()\n",
        "        \n",
        "        self.net.save(NET_PATH)\n",
        "        self.rand_pred.save(PRED_PATH)\n",
        "        if USE_GPU:\n",
        "            self.net.cuda()\n",
        "            self.rand_pred.cuda()\n",
        "            \n",
        "    def load_model(self):\n",
        "        self.net.cpu()\n",
        "        self.rand_pred.cpu()\n",
        "        \n",
        "        self.net.load(NET_PATH)\n",
        "        self.rand_pred.load(PRED_PATH)\n",
        "        if USE_GPU:\n",
        "            self.net.cuda()\n",
        "            self.rand_pred.cuda()\n",
        "        \n",
        "    def choose_action(self, x):\n",
        "        self.memory_counter += 1\n",
        "        # Assume that x is a np.array of shape (nenvs, 4, 84, 84)\n",
        "        x = torch.FloatTensor(x)\n",
        "        if USE_GPU:\n",
        "            x = x.cuda()\n",
        "        # get action log probs and state values\n",
        "        action_log_probs, state_values, int_state_values = self.net(x) # (nenvs, N_ACTIONS)\n",
        "        probs = F.softmax(action_log_probs, dim=1).data.cpu().numpy()\n",
        "        # sample actions\n",
        "        actions = np.array([np.random.choice(N_ACTIONS,p=probs[i]) for i in range(len(probs))])\n",
        "        # convert tensor to np.array\n",
        "        action_log_probs = action_log_probs.data.cpu().numpy()\n",
        "        state_values = state_values.squeeze(1).data.cpu().numpy()\n",
        "        int_state_values = int_state_values.squeeze(1).data.cpu().numpy()\n",
        "        # calc selected logprob\n",
        "        selected_log_probs = np.array([action_log_probs[i][actions[i]] for i in range(len(probs))])\n",
        "        return actions, state_values, int_state_values, selected_log_probs\n",
        "    \n",
        "    def r_int(self, s):\n",
        "        s = torch.FloatTensor(s)\n",
        "        # feature normalization part in RND\n",
        "        # get intrinsic reward\n",
        "#         r_input = list()\n",
        "#         for i in range(len(s)):\n",
        "#             r_input.append((s[i, -1] - self.s_mu)/(self.s_sigma + 1e-8))\n",
        "#         s = torch.clamp(torch.FloatTensor(r_input).unsqueeze(1), -5., 5.) # (N_ENVS, 1, 84, 84)\n",
        "        if USE_GPU:\n",
        "            s = s.cuda()\n",
        "\n",
        "        # Aufgabe 2\n",
        "        r_target = 0 # (N_ENVS, 256)\n",
        "        r_pred = 0 # (N_ENVS, 256)\n",
        "        r_int = torch.zeros(N_ENVS)\n",
        "        \n",
        "        return r_int.data.cpu().numpy() # (N_Envs)\n",
        "    \n",
        "    def learn_predict(self, s):\n",
        "        s = torch.FloatTensor(s)\n",
        "        # feature normalization part in RND\n",
        "        # RND pred net optimize\n",
        "#         r_input = list()\n",
        "#         for i in range(len(s)):\n",
        "#             r_input.append((obs[i, -1] - self.s_mu)/(self.s_sigma + 1e-8))\n",
        "#         s = torch.clamp(torch.FloatTensor(r_input).unsqueeze(1), -5., 5.) # (N_ENVS, 1, 84, 84)\n",
        "        if USE_GPU:\n",
        "            s = s.cuda()\n",
        "        s.requires_grad = True\n",
        "        r_target = self.rand_target(s) # (N_ENVS, 256)\n",
        "        r_pred = self.rand_pred(s) # (N_ENVS, 256)\n",
        "        r_int = torch.mean(F.mse_loss(r_pred, r_target, reduction='none'), dim=1)\n",
        "        # (N_ENVS)\n",
        "        # zero-centered gradient penalty for vanishing gradient problem. You can remove this part\n",
        "        # check https://arxiv.org/abs/1801.04406 for more information.\n",
        "        grad = autograd.grad(r_int, s, create_graph=True,\n",
        "                        grad_outputs=torch.ones_like(r_int),\n",
        "                        retain_graph=True, only_inputs=True)[0].view(len(s), -1)\n",
        "        grad = grad.norm(dim=1)\n",
        "        loss = r_int.mean() + 100.0 * ((grad)**2).mean()\n",
        "        \n",
        "        self.rand_pred_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.rand_pred.parameters(), MAX_GRAD_NORM)\n",
        "        self.rand_pred_opt.step()\n",
        "        return r_int.data.cpu().numpy()\n",
        "\n",
        "    def learn(self, obs, returns, int_returns, masks, actions, values, int_values,  selected_log_probs):\n",
        "        # np.array -> torch.Tensor\n",
        "        obs = torch.FloatTensor(obs) # (m, 4, 84, 84)\n",
        "        returns = torch.FloatTensor(returns) # (m)\n",
        "        int_returns = torch.FloatTensor(int_returns) # (m)\n",
        "        actions = torch.LongTensor(actions) # (m)\n",
        "        selected_log_probs = torch.FloatTensor(selected_log_probs) # (m)\n",
        "        values = torch.FloatTensor(values) # (m)\n",
        "        int_values = torch.FloatTensor(int_values) # (m)\n",
        "        if USE_GPU:\n",
        "            obs = obs.cuda()\n",
        "            returns = returns.cuda()\n",
        "            int_returns = int_returns.cuda()\n",
        "            actions = actions.cuda()\n",
        "            selected_log_probs = selected_log_probs.cuda()\n",
        "            values = values.cuda()\n",
        "            int_values = int_values.cuda()\n",
        "        \n",
        "        # get action log probs and state values\n",
        "        action_log_probs, state_values, int_state_values = self.net(obs)\n",
        "        # (m, N_ACTIONS), (m, 1)\n",
        "        \n",
        "        # calculate the advantages\n",
        "        # original RND\n",
        "#         advs = 2 * (returns - values) + (int_returns - int_values)\n",
        "        # only intrinsic motivation agent's advantage\n",
        "        advs = (int_returns - int_values)\n",
        "        advs = (advs - advs.mean())/(advs.std() + 1e-8)\n",
        "        \n",
        "        # calc probs\n",
        "        probs = F.softmax(action_log_probs, dim=1)\n",
        "        # (m, N_ACTIONS)\n",
        "        \n",
        "        # calc entropy loss\n",
        "        ent_loss = ENT_COEF *((action_log_probs * probs).sum(dim=1)).mean()\n",
        "        # (1)\n",
        "        \n",
        "        # calc log probs\n",
        "        cur_log_probs = action_log_probs.gather(1,actions.unsqueeze(1))\n",
        "        # cur : (m, 1)\n",
        "        ratio = torch.exp(cur_log_probs.squeeze(1)-selected_log_probs)\n",
        "        # (m)\n",
        "        \n",
        "        # actor loss\n",
        "        surr1 = ratio * advs # (m)\n",
        "        surr2 = torch.clamp(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range)*advs # (m)\n",
        "        actor_loss = -torch.min(surr1, surr2).mean() # (1)\n",
        "        # critic loss\n",
        "        critic_loss = F.smooth_l1_loss(state_values.squeeze(1), returns) # (1)\n",
        "        # int critic loss\n",
        "        int_critic_loss = F.smooth_l1_loss(int_state_values.squeeze(1), int_returns) # (1)\n",
        "\n",
        "        loss = actor_loss + critic_loss + ent_loss + int_critic_loss # (1)\n",
        "        \n",
        "        actor_loss, critic_loss, ent_loss, total_loss = actor_loss.data.cpu().numpy(), \\\n",
        "        critic_loss.data.cpu().numpy(), ent_loss.data.cpu().numpy(), loss.data.cpu().numpy()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.net.parameters(), MAX_GRAD_NORM)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return round(float(actor_loss), 4), round(float(critic_loss), 4),\\\n",
        "    round(float(ent_loss), 4), round(float(total_loss), 4)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OzTFMTsb1gN"
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class AbstractEnvRunner(ABC):\n",
        "    def __init__(self, *, env, model, nsteps):\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.nenv = nenv = env.num_envs if hasattr(env, 'num_envs') else 1\n",
        "        self.batch_ob_shape = (nenv*nsteps,) + env.observation_space.shape\n",
        "        self.obs = np.zeros((nenv,) + env.observation_space.shape, dtype=env.observation_space.dtype.name)\n",
        "        self.obs[:] = env.reset()\n",
        "        self.nsteps = nsteps\n",
        "        self.dones = [False for _ in range(nenv)]\n",
        "\n",
        "    @abstractmethod\n",
        "    def run(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Runner(AbstractEnvRunner):\n",
        "    \"\"\"\n",
        "    We use this object to make a mini batch of experiences\n",
        "    __init__:\n",
        "    - Initialize the runner\n",
        "    run():\n",
        "    - Make a mini batch\n",
        "    \"\"\"\n",
        "    def __init__(self, *, env, model, nsteps, gamma, int_gamma, lam, rnd_start=int(1e+3)):\n",
        "        super().__init__(env=env, model=model, nsteps=nsteps)\n",
        "        # Lambda used in GAE (General Advantage Estimation)\n",
        "        self.lam = lam\n",
        "        # Discount rate\n",
        "        self.gamma = gamma\n",
        "        self.int_gamma = int_gamma\n",
        "        self.rnd_start = rnd_start\n",
        "        self.s_arr = list()\n",
        "\n",
        "    def run(self):\n",
        "        # Here, we init the lists that will contain the mb of experiences\n",
        "        mb_obs, mb_rewards, mb_int_rewards, mb_actions, mb_values, mb_int_values, mb_dones, mb_neglogpacs = [],[],[],[],[],[], [], []\n",
        "        epinfos = []\n",
        "        # For n in range number of steps\n",
        "        for _ in range(self.nsteps):\n",
        "            # Given observations, get action value and neglopacs\n",
        "            # We already have self.obs because Runner superclass run self.obs[:] = env.reset() on init\n",
        "            actions, values, int_values, neglogpacs = self.model.choose_action(self.obs)\n",
        "            mb_obs.append(self.obs.copy())\n",
        "            mb_actions.append(actions)\n",
        "            mb_values.append(values)\n",
        "            mb_int_values.append(int_values)\n",
        "            mb_neglogpacs.append(neglogpacs)\n",
        "            mb_dones.append(self.dones)\n",
        "\n",
        "            # Take actions in env and look the results\n",
        "            # Infos contains a ton of useful informations\n",
        "            self.obs[:], rewards, self.dones, infos = self.env.step(actions)\n",
        "\n",
        "            if self.model.memory_counter < self.rnd_start:\n",
        "                # RND state data gather\n",
        "                for i in range(len(self.obs)):\n",
        "                    # RND uses only last image\n",
        "                    self.s_arr.append(self.obs[i, -1])\n",
        "                r_int = np.zeros_like(rewards)\n",
        "            elif self.model.memory_counter == self.rnd_start:\n",
        "                print(\"RND STAT FINISH\")\n",
        "                # calc state stat\n",
        "                self.model.s_mu = np.mean(self.s_arr, axis=0)\n",
        "                self.model.s_sigma = np.std(self.s_arr, axis=0)\n",
        "                r_int = np.zeros_like(rewards)\n",
        "            else:\n",
        "                # get intrinsic reward\n",
        "                r_int = self.model.r_int(self.obs)\n",
        "                    \n",
        "                   \n",
        "            mb_rewards.append(rewards)\n",
        "            mb_int_rewards.append(r_int)\n",
        "        #batch of steps to batch of rollouts\n",
        "        mb_obs = np.asarray(mb_obs, dtype=self.obs.dtype)\n",
        "        mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\n",
        "        mb_int_rewards = np.asarray(mb_int_rewards, dtype=np.float32)\n",
        "        mb_actions = np.asarray(mb_actions)\n",
        "        mb_values = np.asarray(mb_values, dtype=np.float32)\n",
        "        mb_int_values = np.asarray(mb_int_values, dtype=np.float32)\n",
        "        mb_neglogpacs = np.asarray(mb_neglogpacs, dtype=np.float32)\n",
        "        mb_dones = np.asarray(mb_dones, dtype=np.bool)\n",
        "        \n",
        "        # Aufgabe 3\n",
        "        # post processing of intrinsic rewards\n",
        "        mb_int_rewards = mb_int_rewards\n",
        "        \n",
        "        # choose action then we get (actions, values, log_probs)\n",
        "        last_values = self.model.choose_action(self.obs)[1]\n",
        "        last_int_values = self.model.choose_action(self.obs)[2]\n",
        "\n",
        "        # discount/bootstrap off value fn\n",
        "        mb_returns = np.zeros_like(mb_rewards)\n",
        "        mb_advs = np.zeros_like(mb_rewards)\n",
        "        lastgaelam = 0\n",
        "        \n",
        "        mb_int_returns = np.zeros_like(mb_int_rewards)\n",
        "        mb_int_advs = np.zeros_like(mb_int_rewards)\n",
        "        int_lastgaelam = 0\n",
        "        \n",
        "        for t in reversed(range(self.nsteps)):\n",
        "            \n",
        "            if t == self.nsteps - 1:\n",
        "                nextnonterminal = 1.0 - self.dones\n",
        "                nextvalues = last_values\n",
        "                next_intvalues = last_int_values\n",
        "            else:\n",
        "                nextnonterminal = 1.0 - mb_dones[t+1]\n",
        "                nextvalues = mb_values[t+1]\n",
        "                next_intvalues = mb_int_values[t+1]\n",
        "                \n",
        "            delta = mb_rewards[t] + self.gamma * nextvalues * nextnonterminal - mb_values[t]\n",
        "            mb_advs[t] = lastgaelam = delta + self.gamma * self.lam * nextnonterminal * lastgaelam\n",
        "            \n",
        "            int_delta = mb_int_rewards[t] + self.int_gamma * next_intvalues * nextnonterminal - mb_int_values[t]\n",
        "            mb_int_advs[t] = int_lastgaelam = int_delta + self.int_gamma * self.lam * nextnonterminal * int_lastgaelam\n",
        "        \n",
        "        mb_returns = mb_advs + mb_values\n",
        "        mb_int_returns = mb_int_advs + mb_int_values\n",
        "\n",
        "\n",
        "        return (*map(sf01, (mb_obs, mb_returns, mb_int_rewards, mb_int_returns, mb_dones, mb_actions, mb_values, mb_int_values,  mb_neglogpacs)),\n",
        "            np.sum(mb_rewards, axis = 0))\n",
        "        \n",
        "def sf01(arr):\n",
        "    \"\"\"\n",
        "    swap and then flatten axes 0 and 1\n",
        "    \"\"\"\n",
        "    s = arr.shape\n",
        "    return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc-vyPNr9Mc7"
      },
      "source": [
        "## Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gFhN_7UB9Mc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "dff96d6c-ee72-442f-fede-b7d97b4ed0fd"
      },
      "source": [
        "ppo = PPO()\n",
        "runner = Runner(env=env, model=ppo, nsteps=TRAJ_LEN, gamma=GAMMA, int_gamma=INT_GAMMA, lam=LAMBDA, rnd_start=RND_START)\n",
        "\n",
        "# model load with check\n",
        "if LOAD and os.path.isfile(PRED_PATH) and os.path.isfile(TARGET_PATH):\n",
        "    ppo.load_model()\n",
        "    pkl_file = open(RESULT_PATH,'rb')\n",
        "    result = pickle.load(pkl_file)\n",
        "    pkl_file.close()\n",
        "    print('Load complete!')\n",
        "else:\n",
        "    result = []\n",
        "    print('Initialize results!')\n",
        "\n",
        "print('Collecting experience...')\n",
        "\n",
        "# episode step for accumulate reward \n",
        "epinfobuf = deque(maxlen=100)\n",
        "# in PPO, we iterate over optimization step\n",
        "nbatch = N_ENVS * TRAJ_LEN\n",
        "nupdates = N_STEP// nbatch\n",
        "# check learning time\n",
        "start_time = time.time()\n",
        "\n",
        "for update in tqdm(range(1, nupdates+1)):\n",
        "    # get minibatch\n",
        "    obs, returns, int_rewards, int_returns, masks, actions, values, int_values, neglogpacs, rewards = runner.run()\n",
        "    epinfobuf.append(rewards.sum())\n",
        "    \n",
        "    if ppo.memory_counter > RND_START:\n",
        "        # calculate loss\n",
        "        inds = np.arange(nbatch)\n",
        "        for _ in range(N_OPT_EPOCHS):\n",
        "            a_losses, c_losses, e_losses, t_losses = list(), list(), list(), list()\n",
        "            # shuffle indices for i.i.d.\n",
        "            np.random.shuffle(inds)\n",
        "            # 0 to batch_size with batch_train_size step\n",
        "            for start in range(0, nbatch, BATCH_SIZE):\n",
        "                end = start + BATCH_SIZE\n",
        "                mbinds = inds[start:end]\n",
        "                slices = (arr[mbinds] for arr in (obs, returns, int_returns, masks, actions, values, int_values, neglogpacs))\n",
        "                actor_loss, critic_loss, ent_loss, total_loss = ppo.learn(*slices)\n",
        "                if np.random.rand() <= 0.25:\n",
        "                    ppo.learn_predict(obs[mbinds])\n",
        "            \n",
        "        if update % LOG_FREQ == 0:\n",
        "            # print log and save\n",
        "            # check time interval\n",
        "            time_interval = round(time.time() - start_time, 2)\n",
        "            # calc mean return\n",
        "            mean_100_ep_return = round(np.mean(list(epinfobuf)),2)\n",
        "            result.append(mean_100_ep_return)\n",
        "            # print epi log\n",
        "            print('N : ',update,\n",
        "                  '| Return mean: ', mean_100_ep_return,\n",
        "                  '| R_int_mean : ', round(np.mean(int_rewards),3),\n",
        "                  '| R_int_std : ', round(np.std(int_rewards),3),\n",
        "                  '| Values : ', round(np.mean(values), 3),\n",
        "                  '| Int_values : ', round(np.mean(int_values), 3),\n",
        "                  '| Time:',time_interval,\n",
        "                  '| Used Step:',ppo.memory_counter*N_ENVS)\n",
        "            # save model\n",
        "            if SAVE:\n",
        "                ppo.save_model()\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/19531 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initialize results!\n",
            "Collecting experience...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 10/19531 [00:26<14:15:17,  2.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "N :  10 | Return mean:  3.4 | R_int_mean :  0.0 | R_int_std :  0.0 | Values :  0.347 | Int_values :  0.029 | Time: 26.42 | Used Step: 5200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 20/19531 [00:51<13:51:33,  2.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "N :  20 | Return mean:  3.6 | R_int_mean :  0.0 | R_int_std :  0.0 | Values :  0.308 | Int_values :  0.03 | Time: 51.82 | Used Step: 10400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 30/19531 [01:16<13:31:37,  2.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "N :  30 | Return mean:  3.63 | R_int_mean :  0.0 | R_int_std :  0.0 | Values :  0.169 | Int_values :  0.008 | Time: 76.78 | Used Step: 15600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 40/19531 [01:42<14:17:42,  2.64s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "N :  40 | Return mean:  3.7 | R_int_mean :  0.0 | R_int_std :  0.0 | Values :  0.461 | Int_values :  0.031 | Time: 102.41 | Used Step: 20800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 50/19531 [02:06<13:33:45,  2.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "N :  50 | Return mean:  4.1 | R_int_mean :  0.0 | R_int_std :  0.0 | Values :  0.605 | Int_values :  0.002 | Time: 126.88 | Used Step: 26000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 60/19531 [02:32<14:00:49,  2.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "N :  60 | Return mean:  4.12 | R_int_mean :  0.0 | R_int_std :  0.0 | Values :  0.22 | Int_values :  0.02 | Time: 152.25 | Used Step: 31200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 70/19531 [02:56<13:16:46,  2.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "N :  70 | Return mean:  4.14 | R_int_mean :  0.0 | R_int_std :  0.0 | Values :  0.279 | Int_values :  -0.009 | Time: 176.92 | Used Step: 36400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 80/19531 [03:22<13:25:35,  2.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "N :  80 | Return mean:  4.15 | R_int_mean :  0.0 | R_int_std :  0.0 | Values :  0.223 | Int_values :  0.014 | Time: 202.39 | Used Step: 41600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 81/19531 [03:24<13:26:55,  2.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4f203f64e45e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mmbinds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmbinds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmbinds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4f203f64e45e>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mmbinds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmbinds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfwd3Ynu9MdB"
      },
      "source": [
        "## Plot extrinsic reward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gUnSea-d9MdB"
      },
      "source": [
        "plt.plot(range(len(result)), result)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFieikFFp0sz"
      },
      "source": [
        "## Watch the agent playing!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdoR9p2v9MdC"
      },
      "source": [
        "from matplotlib import animation\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "        \n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
        "    anim.save('rnd_breakout_result.gif', writer='imagemagick', fps=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4_mgM7t9MdC"
      },
      "source": [
        "# Aufgabe 4\n",
        "\n",
        "env = wrap_deepmind(make_atari(ENV_NAME))\n",
        "s = np.array(env.reset())\n",
        "total_reward = 0\n",
        "frames = []\n",
        "done_counter = 0\n",
        "\n",
        "for t in range(10000):\n",
        "    # Render into buffer. \n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    a, v, int_v, l = ppo.choose_action(np.expand_dims(s,axis=0))\n",
        "    # take action and get next state\n",
        "    s_, r, done, info = env.step(a)\n",
        "    s_ = np.array(s_)\n",
        "    total_reward += r\n",
        "    if done:\n",
        "        done_counter += 1\n",
        "        if done_counter == 5:\n",
        "            break\n",
        "    s = s_\n",
        "env.close()\n",
        "print('Total Reward : %.2f'%total_reward)\n",
        "anim = display_frames_as_gif(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRnbnkKXjiZW"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(open('rnd_breakout_result.gif','rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8qqGRHq9MdD"
      },
      "source": [
        "![alt text](./rnd_breakout_result.gif \"segment\")"
      ]
    }
  ]
}