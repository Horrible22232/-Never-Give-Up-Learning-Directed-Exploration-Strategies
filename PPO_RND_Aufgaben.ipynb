{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "PPO_RND_Aufgaben.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ytOBAt6eCEy"
      },
      "source": [
        "# Aufgaben\n",
        "\n",
        "Die Stellen, an denen Aufgaben gelöst werden sollen, sind im Code mit Kommentaren versehen.\n",
        "\n",
        "1) Definiert einen passenden Dense Layer im RandomTargetNetwork.\n",
        "\n",
        "2) Definiert eine mögliche Berechnung der intrinsischen Belohnung. \n",
        "\n",
        "3) Normalisiere die intrinsische Belohnung, sodass die Skalierung dieser unabhängig von der aktuellen Umwelt ist. Folgende Quelle könnte dabei hilfreich sein: [RND Paper](https://arxiv.org/pdf/1810.12894.pdf) (S.5)\n",
        "\n",
        "4) Experimentiert:\n",
        "* Entferne die extrinsische Belohnung und schaue, wie sich der Agent dann verhält.\n",
        "* Baue eine andere intrinsische Belohnung.\n",
        "* Versuche den Agenten auf einem anderen Environment zu Trainieren.\n",
        "* (Baue den NGU Agenten.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxxu8H-N9Mcm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fp2GnwWT7--",
        "outputId": "ed9061b4-8cf9-489e-bb29-f349bcd6d711"
      },
      "source": [
        "!git clone https://github.com/openai/baselines.git\n",
        "!apt-get install imagemagick"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'baselines'...\n",
            "remote: Enumerating objects: 3627, done.\u001b[K\n",
            "remote: Total 3627 (delta 0), reused 0 (delta 0), pack-reused 3627\u001b[K\n",
            "Receiving objects: 100% (3627/3627), 6.46 MiB | 24.24 MiB/s, done.\n",
            "Resolving deltas: 100% (2429/2429), done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono ghostscript gsfonts\n",
            "  imagemagick-6-common imagemagick-6.q16 libcupsfilters1 libcupsimage2\n",
            "  libdjvulibre-text libdjvulibre21 libgs9 libgs9-common libijs-0.35\n",
            "  libjbig2dec0 liblqr-1-0 libmagickcore-6.q16-3 libmagickcore-6.q16-3-extra\n",
            "  libmagickwand-6.q16-3 libnetpbm10 libwmf0.2-7 netpbm poppler-data\n",
            "Suggested packages:\n",
            "  fonts-noto ghostscript-x imagemagick-doc autotrace cups-bsd | lpr | lprng\n",
            "  enscript gimp gnuplot grads hp2xx html2ps libwmf-bin mplayer povray radiance\n",
            "  sane-utils texlive-base-bin transfig ufraw-batch inkscape libjxr-tools\n",
            "  libwmf0.2-7-gtk poppler-utils fonts-japanese-mincho | fonts-ipafont-mincho\n",
            "  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
            "  fonts-arphic-uming fonts-nanum\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono ghostscript gsfonts imagemagick\n",
            "  imagemagick-6-common imagemagick-6.q16 libcupsfilters1 libcupsimage2\n",
            "  libdjvulibre-text libdjvulibre21 libgs9 libgs9-common libijs-0.35\n",
            "  libjbig2dec0 liblqr-1-0 libmagickcore-6.q16-3 libmagickcore-6.q16-3-extra\n",
            "  libmagickwand-6.q16-3 libnetpbm10 libwmf0.2-7 netpbm poppler-data\n",
            "0 upgraded, 23 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 18.4 MB of archives.\n",
            "After this operation, 66.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblqr-1-0 amd64 0.4.2-2.1 [27.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 imagemagick-6-common all 8:6.9.7.4+dfsg-16ubuntu6.9 [60.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagickcore-6.q16-3 amd64 8:6.9.7.4+dfsg-16ubuntu6.9 [1,616 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagickwand-6.q16-3 amd64 8:6.9.7.4+dfsg-16ubuntu6.9 [293 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 poppler-data all 0.4.8-2 [1,479 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-noto-mono all 20171026-2 [75.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsimage2 amd64 2.2.7-1ubuntu2.8 [18.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libijs-0.35 amd64 0.35-13 [15.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig2dec0 amd64 0.13-6 [55.9 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9-common all 9.26~dfsg+0-0ubuntu0.18.04.14 [5,092 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9 amd64 9.26~dfsg+0-0ubuntu0.18.04.14 [2,265 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ghostscript amd64 9.26~dfsg+0-0ubuntu0.18.04.14 [51.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.4 [3,120 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 imagemagick-6.q16 amd64 8:6.9.7.4+dfsg-16ubuntu6.9 [423 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 imagemagick amd64 8:6.9.7.4+dfsg-16ubuntu6.9 [14.2 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsfilters1 amd64 1.20.2-0ubuntu3.1 [108 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdjvulibre-text all 3.5.27.1-8ubuntu0.2 [49.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdjvulibre21 amd64 3.5.27.1-8ubuntu0.2 [560 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwmf0.2-7 amd64 0.2.8.4-12 [150 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagickcore-6.q16-3-extra amd64 8:6.9.7.4+dfsg-16ubuntu6.9 [62.3 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnetpbm10 amd64 2:10.0-15.3build1 [58.0 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 netpbm amd64 2:10.0-15.3build1 [1,017 kB]\n",
            "Fetched 18.4 MB in 2s (7,527 kB/s)\n",
            "debconf: apt-extracttemplates failed: No such file or directory\n",
            "E: Sub-process /usr/sbin/dpkg-preconfigure --apt || true received signal 2.\n",
            "E: Failure running script /usr/sbin/dpkg-preconfigure --apt || true\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ph1J_CP9Mcu"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, 'baselines')\n",
        "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUqlvW1bfM73"
      },
      "source": [
        "## Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU1u94nASTPF"
      },
      "source": [
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done  = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
        "            # so it's important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip       = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n",
        "        \"\"\"\n",
        "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n",
        "        observation should be warped.\n",
        "        \"\"\"\n",
        "        super().__init__(env)\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grayscale = grayscale\n",
        "        self._key = dict_space_key\n",
        "        if self._grayscale:\n",
        "            num_colors = 1\n",
        "        else:\n",
        "            num_colors = 3\n",
        "\n",
        "        new_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=(self._height, self._width, num_colors),\n",
        "            dtype=np.uint8,\n",
        "        )\n",
        "        if self._key is None:\n",
        "            original_space = self.observation_space\n",
        "            self.observation_space = new_space\n",
        "        else:\n",
        "            original_space = self.observation_space.spaces[self._key]\n",
        "            self.observation_space.spaces[self._key] = new_space\n",
        "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
        "\n",
        "    def observation(self, obs):\n",
        "        if self._key is None:\n",
        "            frame = obs\n",
        "        else:\n",
        "            frame = obs[self._key]\n",
        "\n",
        "        if self._grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(\n",
        "            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "        if self._grayscale:\n",
        "            frame = np.expand_dims(frame, -1)\n",
        "\n",
        "        if self._key is None:\n",
        "            obs = frame\n",
        "        else:\n",
        "            obs = obs.copy()\n",
        "            obs[self._key] = frame\n",
        "        return obs\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Change image shape to CWH\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.swapaxes(observation, 2, 0)\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "        self._out = None\n",
        "\n",
        "    def _force(self):\n",
        "        if self._out is None:\n",
        "            self._out = np.concatenate(self._frames, axis=-1)\n",
        "            self._frames = None\n",
        "        return self._out\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = self._force()\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._force())\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._force()[i]\n",
        "\n",
        "    def count(self):\n",
        "        frames = self._force()\n",
        "        return frames.shape[frames.ndim - 1]\n",
        "\n",
        "    def frame(self, i):\n",
        "        return self._force()[..., i]\n",
        "\n",
        "def make_atari(env_id, max_episode_steps=None):\n",
        "    env = gym.make(env_id)\n",
        "    assert 'NoFrameskip' in env.spec.id\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    if max_episode_steps is not None:\n",
        "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
        "    return env\n",
        "\n",
        "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=True, scale=False):\n",
        "    \"\"\"Configure environment for DeepMind-style Atari.\n",
        "    \"\"\"\n",
        "    if episode_life:\n",
        "        env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    if scale:\n",
        "        env = ScaledFloatFrame(env)\n",
        "    if clip_rewards:\n",
        "        env = ClipRewardEnv(env)\n",
        "    if frame_stack:\n",
        "        env = FrameStack(env, 4)\n",
        "    return ImageToPyTorch(env)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYoTfrgh9Mcw"
      },
      "source": [
        "## Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7_ZB3xcC9Mcw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22051c86-6cbb-40a0-dedd-de6293bf8b50"
      },
      "source": [
        "'PPO Settings'''\n",
        "TRAJ_LEN = 128\n",
        "N_OPT_EPOCHS = 10\n",
        "ENT_COEF = 1e-2\n",
        "CLIP_RANGE = 0.1\n",
        "LAMBDA = 0.95\n",
        "\n",
        "'''RND Settings'''\n",
        "# RND start step for input normalization\n",
        "RND_START = int(0)\n",
        "# Discount rate for intrinsic reward\n",
        "INT_GAMMA = 0.99\n",
        "\n",
        "'''Environment Settings'''\n",
        "# sequential images to define state\n",
        "STATE_LEN = 4\n",
        "# openai gym env name\n",
        "ENV_NAME = 'BreakoutNoFrameskip-v4'\n",
        "# number of environments for A2C\n",
        "N_ENVS = 4\n",
        "# define gym \n",
        "env = DummyVecEnv([lambda: wrap_deepmind(make_atari(ENV_NAME)) for i in range(N_ENVS)])\n",
        "\n",
        "# check gym setting\n",
        "N_ACTIONS = env.action_space.n;print('N_ACTIONS : ', N_ACTIONS) #  6\n",
        "N_STATES = env.observation_space.shape;print('N_STATES : ', N_STATES) # (4, 84, 84)\n",
        "# Total simulation step\n",
        "N_STEP = int(1e+7)\n",
        "# gamma for MDP\n",
        "GAMMA = 0.9999\n",
        "# visualize for agent playing\n",
        "RENDERING = False\n",
        "\n",
        "'''Training settings'''\n",
        "# check GPU usage\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "print('USE GPU: '+str(USE_GPU))\n",
        "# mini-batch size\n",
        "BATCH_SIZE = 128\n",
        "# learning rage\n",
        "LR = 1e-4\n",
        "# clip gradient\n",
        "MAX_GRAD_NORM = 0.1\n",
        "# log optimization\n",
        "LOG_OPT = False\n",
        "\n",
        "'''Save&Load Settings'''\n",
        "# log frequency\n",
        "LOG_FREQ = 10\n",
        "# check save/load\n",
        "SAVE = False\n",
        "LOAD = False\n",
        "# paths for predction net, target net, result log\n",
        "NET_PATH = './data/model/ppo_net.pkl'\n",
        "PRED_PATH = './data/model/pred_net.pkl'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N_ACTIONS :  4\n",
            "N_STATES :  (4, 84, 84)\n",
            "USE GPU: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcGeVltC9Mcy"
      },
      "source": [
        "## Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFX_bEIT9Mcy"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # architecture def\n",
        "        self.feature_extraction = nn.Sequential(\n",
        "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        # actor\n",
        "        self.actor = nn.Linear(256, N_ACTIONS)\n",
        "        # extrinsic critic\n",
        "        self.critic = nn.Linear(256, 1)\n",
        "        # intrinsic critic\n",
        "        self.int_critic = nn.Linear(256, 1)\n",
        "            \n",
        "        # parameter initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a tensor of (m, 4, 84, 84)\n",
        "        x = self.feature_extraction(x / 255.0)\n",
        "        # x.size(0) : mini-batch size\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(F.relu(x))\n",
        "        x = self.fc2(F.relu(x))\n",
        "        # use log_softmax for numerical stability\n",
        "        action_log_prob = F.log_softmax(self.actor(F.relu(x)), dim=1)\n",
        "        state_value = self.critic(F.relu(x))\n",
        "        int_state_value = self.int_critic(F.relu(x))\n",
        "\n",
        "        return action_log_prob, state_value, int_state_value\n",
        "\n",
        "    def save(self, PATH):\n",
        "        torch.save(self.state_dict(),PATH)\n",
        "\n",
        "    def load(self, PATH):\n",
        "        self.load_state_dict(torch.load(PATH))\n",
        "        \n",
        "class RandomPredNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RandomPredNet, self).__init__()\n",
        "        # architecture def\n",
        "        self.feature_extraction = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
        "            nn.LeakyReLU(negative_slope=2e-1),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.LeakyReLU(negative_slope=2e-1),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 256)\n",
        "        # one more layer than target network for enough capacity\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            \n",
        "\n",
        "    def forward(self, x):\n",
        "        # if you use feature normalization in RND, remove division by 255.0\n",
        "        x = self.feature_extraction(x / 255.0)\n",
        "        # x.size(0) : mini-batch size\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(F.leaky_relu(x, negative_slope=2e-1))\n",
        "        x = self.fc2(F.leaky_relu(x, negative_slope=2e-1))\n",
        "        return x\n",
        "    \n",
        "    def save(self, PATH):\n",
        "        torch.save(self.state_dict(),PATH)\n",
        "\n",
        "    def load(self, PATH):\n",
        "        self.load_state_dict(torch.load(PATH))\n",
        "        \n",
        "class RandomTargetNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RandomTargetNet, self).__init__()\n",
        "        # architecture def\n",
        "        self.feature_extraction = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "        )\n",
        "\n",
        "        # Aufgabe 1\n",
        "        self.fc1 = lambda x: torch.zeros((x.size(0), 256)).to(\"cuda\")\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # if you use feature normalization in RND, remove division by 255.0\n",
        "        x = self.feature_extraction(x / 255.0)\n",
        "        # x.size(0) : mini-batch size\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(F.relu(x))\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVUvCmGa9Mc3"
      },
      "source": [
        "## RND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY292hJZ9Mc5"
      },
      "source": [
        "class PPO:\n",
        "    def __init__(self):\n",
        "        self.net = ConvNet()\n",
        "        self.rand_target = RandomTargetNet()\n",
        "        self.rand_pred = RandomPredNet()\n",
        "        # use gpu\n",
        "        if USE_GPU:\n",
        "            self.net = self.net.cuda()\n",
        "            self.rand_target = self.rand_target.cuda()\n",
        "            self.rand_pred = self.rand_pred.cuda()\n",
        "            \n",
        "        # simulator step conter\n",
        "        self.memory_counter = 0\n",
        "        \n",
        "        # define optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=LR)\n",
        "        # define optimizer for predict network\n",
        "        self.rand_pred_opt = torch.optim.Adam(self.rand_pred.parameters(), lr=LR)\n",
        "        \n",
        "        # ppo clip range\n",
        "        self.clip_range = CLIP_RANGE\n",
        "        \n",
        "        # observation statistics for RND (if you use feature normalization in RND)\n",
        "        self.s_mu = None\n",
        "        self.s_sigma = None\n",
        "        \n",
        "    def save_model(self):\n",
        "        self.net.cpu()\n",
        "        self.rand_pred.cpu()\n",
        "        \n",
        "        self.net.save(NET_PATH)\n",
        "        self.rand_pred.save(PRED_PATH)\n",
        "        if USE_GPU:\n",
        "            self.net.cuda()\n",
        "            self.rand_pred.cuda()\n",
        "            \n",
        "    def load_model(self):\n",
        "        self.net.cpu()\n",
        "        self.rand_pred.cpu()\n",
        "        \n",
        "        self.net.load(NET_PATH)\n",
        "        self.rand_pred.load(PRED_PATH)\n",
        "        if USE_GPU:\n",
        "            self.net.cuda()\n",
        "            self.rand_pred.cuda()\n",
        "        \n",
        "    def choose_action(self, x):\n",
        "        self.memory_counter += 1\n",
        "        # Assume that x is a np.array of shape (nenvs, 4, 84, 84)\n",
        "        x = torch.FloatTensor(x)\n",
        "        if USE_GPU:\n",
        "            x = x.cuda()\n",
        "        # get action log probs and state values\n",
        "        action_log_probs, state_values, int_state_values = self.net(x) # (nenvs, N_ACTIONS)\n",
        "        probs = F.softmax(action_log_probs, dim=1).data.cpu().numpy()\n",
        "        # sample actions\n",
        "        actions = np.array([np.random.choice(N_ACTIONS,p=probs[i]) for i in range(len(probs))])\n",
        "        # convert tensor to np.array\n",
        "        action_log_probs = action_log_probs.data.cpu().numpy()\n",
        "        state_values = state_values.squeeze(1).data.cpu().numpy()\n",
        "        int_state_values = int_state_values.squeeze(1).data.cpu().numpy()\n",
        "        # calc selected logprob\n",
        "        selected_log_probs = np.array([action_log_probs[i][actions[i]] for i in range(len(probs))])\n",
        "        return actions, state_values, int_state_values, selected_log_probs\n",
        "    \n",
        "    def r_int(self, s):\n",
        "        s = torch.FloatTensor(s)\n",
        "        # feature normalization part in RND\n",
        "        # get intrinsic reward\n",
        "#         r_input = list()\n",
        "#         for i in range(len(s)):\n",
        "#             r_input.append((s[i, -1] - self.s_mu)/(self.s_sigma + 1e-8))\n",
        "#         s = torch.clamp(torch.FloatTensor(r_input).unsqueeze(1), -5., 5.) # (N_ENVS, 1, 84, 84)\n",
        "        if USE_GPU:\n",
        "            s = s.cuda()\n",
        "\n",
        "        # Aufgabe 2\n",
        "        r_target = 0 # (N_ENVS, 256)\n",
        "        r_pred = 0 # (N_ENVS, 256)\n",
        "        r_int = torch.zeros(N_ENVS)\n",
        "        \n",
        "        return r_int.data.cpu().numpy() # (N_Envs)\n",
        "    \n",
        "    def learn_predict(self, s):\n",
        "        s = torch.FloatTensor(s)\n",
        "        # feature normalization part in RND\n",
        "        # RND pred net optimize\n",
        "#         r_input = list()\n",
        "#         for i in range(len(s)):\n",
        "#             r_input.append((obs[i, -1] - self.s_mu)/(self.s_sigma + 1e-8))\n",
        "#         s = torch.clamp(torch.FloatTensor(r_input).unsqueeze(1), -5., 5.) # (N_ENVS, 1, 84, 84)\n",
        "        if USE_GPU:\n",
        "            s = s.cuda()\n",
        "        s.requires_grad = True\n",
        "        r_target = self.rand_target(s) # (N_ENVS, 256)\n",
        "        r_pred = self.rand_pred(s) # (N_ENVS, 256)\n",
        "        r_int = torch.mean(F.mse_loss(r_pred, r_target, reduction='none'), dim=1)\n",
        "        # (N_ENVS)\n",
        "        # zero-centered gradient penalty for vanishing gradient problem. You can remove this part\n",
        "        # check https://arxiv.org/abs/1801.04406 for more information.\n",
        "        grad = autograd.grad(r_int, s, create_graph=True,\n",
        "                        grad_outputs=torch.ones_like(r_int),\n",
        "                        retain_graph=True, only_inputs=True)[0].view(len(s), -1)\n",
        "        grad = grad.norm(dim=1)\n",
        "        loss = r_int.mean() + 100.0 * ((grad)**2).mean()\n",
        "        \n",
        "        self.rand_pred_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.rand_pred.parameters(), MAX_GRAD_NORM)\n",
        "        self.rand_pred_opt.step()\n",
        "        return r_int.data.cpu().numpy()\n",
        "\n",
        "    def learn(self, obs, returns, int_returns, masks, actions, values, int_values,  selected_log_probs):\n",
        "        # np.array -> torch.Tensor\n",
        "        obs = torch.FloatTensor(obs) # (m, 4, 84, 84)\n",
        "        returns = torch.FloatTensor(returns) # (m)\n",
        "        int_returns = torch.FloatTensor(int_returns) # (m)\n",
        "        actions = torch.LongTensor(actions) # (m)\n",
        "        selected_log_probs = torch.FloatTensor(selected_log_probs) # (m)\n",
        "        values = torch.FloatTensor(values) # (m)\n",
        "        int_values = torch.FloatTensor(int_values) # (m)\n",
        "        if USE_GPU:\n",
        "            obs = obs.cuda()\n",
        "            returns = returns.cuda()\n",
        "            int_returns = int_returns.cuda()\n",
        "            actions = actions.cuda()\n",
        "            selected_log_probs = selected_log_probs.cuda()\n",
        "            values = values.cuda()\n",
        "            int_values = int_values.cuda()\n",
        "        \n",
        "        # get action log probs and state values\n",
        "        action_log_probs, state_values, int_state_values = self.net(obs)\n",
        "        # (m, N_ACTIONS), (m, 1)\n",
        "        \n",
        "        # calculate the advantages\n",
        "        # original RND\n",
        "#         advs = 2 * (returns - values) + (int_returns - int_values)\n",
        "        # only intrinsic motivation agent's advantage\n",
        "        advs = (int_returns - int_values)\n",
        "        advs = (advs - advs.mean())/(advs.std() + 1e-8)\n",
        "        \n",
        "        # calc probs\n",
        "        probs = F.softmax(action_log_probs, dim=1)\n",
        "        # (m, N_ACTIONS)\n",
        "        \n",
        "        # calc entropy loss\n",
        "        ent_loss = ENT_COEF *((action_log_probs * probs).sum(dim=1)).mean()\n",
        "        # (1)\n",
        "        \n",
        "        # calc log probs\n",
        "        cur_log_probs = action_log_probs.gather(1,actions.unsqueeze(1))\n",
        "        # cur : (m, 1)\n",
        "        ratio = torch.exp(cur_log_probs.squeeze(1)-selected_log_probs)\n",
        "        # (m)\n",
        "        \n",
        "        # actor loss\n",
        "        surr1 = ratio * advs # (m)\n",
        "        surr2 = torch.clamp(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range)*advs # (m)\n",
        "        actor_loss = -torch.min(surr1, surr2).mean() # (1)\n",
        "        # critic loss\n",
        "        critic_loss = F.smooth_l1_loss(state_values.squeeze(1), returns) # (1)\n",
        "        # int critic loss\n",
        "        int_critic_loss = F.smooth_l1_loss(int_state_values.squeeze(1), int_returns) # (1)\n",
        "\n",
        "        loss = actor_loss + critic_loss + ent_loss + int_critic_loss # (1)\n",
        "        \n",
        "        actor_loss, critic_loss, ent_loss, total_loss = actor_loss.data.cpu().numpy(), \\\n",
        "        critic_loss.data.cpu().numpy(), ent_loss.data.cpu().numpy(), loss.data.cpu().numpy()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.net.parameters(), MAX_GRAD_NORM)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return round(float(actor_loss), 4), round(float(critic_loss), 4),\\\n",
        "    round(float(ent_loss), 4), round(float(total_loss), 4)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OzTFMTsb1gN"
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class AbstractEnvRunner(ABC):\n",
        "    def __init__(self, *, env, model, nsteps):\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.nenv = nenv = env.num_envs if hasattr(env, 'num_envs') else 1\n",
        "        self.batch_ob_shape = (nenv*nsteps,) + env.observation_space.shape\n",
        "        self.obs = np.zeros((nenv,) + env.observation_space.shape, dtype=env.observation_space.dtype.name)\n",
        "        self.obs[:] = env.reset()\n",
        "        self.nsteps = nsteps\n",
        "        self.dones = [False for _ in range(nenv)]\n",
        "\n",
        "    @abstractmethod\n",
        "    def run(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Runner(AbstractEnvRunner):\n",
        "    \"\"\"\n",
        "    We use this object to make a mini batch of experiences\n",
        "    __init__:\n",
        "    - Initialize the runner\n",
        "    run():\n",
        "    - Make a mini batch\n",
        "    \"\"\"\n",
        "    def __init__(self, *, env, model, nsteps, gamma, int_gamma, lam, rnd_start=int(1e+3)):\n",
        "        super().__init__(env=env, model=model, nsteps=nsteps)\n",
        "        # Lambda used in GAE (General Advantage Estimation)\n",
        "        self.lam = lam\n",
        "        # Discount rate\n",
        "        self.gamma = gamma\n",
        "        self.int_gamma = int_gamma\n",
        "        self.rnd_start = rnd_start\n",
        "        self.s_arr = list()\n",
        "\n",
        "    def run(self):\n",
        "        # Here, we init the lists that will contain the mb of experiences\n",
        "        mb_obs, mb_rewards, mb_int_rewards, mb_actions, mb_values, mb_int_values, mb_dones, mb_neglogpacs = [],[],[],[],[],[], [], []\n",
        "        epinfos = []\n",
        "        # For n in range number of steps\n",
        "        for _ in range(self.nsteps):\n",
        "            # Given observations, get action value and neglopacs\n",
        "            # We already have self.obs because Runner superclass run self.obs[:] = env.reset() on init\n",
        "            actions, values, int_values, neglogpacs = self.model.choose_action(self.obs)\n",
        "            mb_obs.append(self.obs.copy())\n",
        "            mb_actions.append(actions)\n",
        "            mb_values.append(values)\n",
        "            mb_int_values.append(int_values)\n",
        "            mb_neglogpacs.append(neglogpacs)\n",
        "            mb_dones.append(self.dones)\n",
        "\n",
        "            # Take actions in env and look the results\n",
        "            # Infos contains a ton of useful informations\n",
        "            self.obs[:], rewards, self.dones, infos = self.env.step(actions)\n",
        "\n",
        "            if self.model.memory_counter < self.rnd_start:\n",
        "                # RND state data gather\n",
        "                for i in range(len(self.obs)):\n",
        "                    # RND uses only last image\n",
        "                    self.s_arr.append(self.obs[i, -1])\n",
        "                r_int = np.zeros_like(rewards)\n",
        "            elif self.model.memory_counter == self.rnd_start:\n",
        "                print(\"RND STAT FINISH\")\n",
        "                # calc state stat\n",
        "                self.model.s_mu = np.mean(self.s_arr, axis=0)\n",
        "                self.model.s_sigma = np.std(self.s_arr, axis=0)\n",
        "                r_int = np.zeros_like(rewards)\n",
        "            else:\n",
        "                # get intrinsic reward\n",
        "                r_int = self.model.r_int(self.obs)\n",
        "                    \n",
        "                   \n",
        "            mb_rewards.append(rewards)\n",
        "            mb_int_rewards.append(r_int)\n",
        "        #batch of steps to batch of rollouts\n",
        "        mb_obs = np.asarray(mb_obs, dtype=self.obs.dtype)\n",
        "        mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\n",
        "        mb_int_rewards = np.asarray(mb_int_rewards, dtype=np.float32)\n",
        "        mb_actions = np.asarray(mb_actions)\n",
        "        mb_values = np.asarray(mb_values, dtype=np.float32)\n",
        "        mb_int_values = np.asarray(mb_int_values, dtype=np.float32)\n",
        "        mb_neglogpacs = np.asarray(mb_neglogpacs, dtype=np.float32)\n",
        "        mb_dones = np.asarray(mb_dones, dtype=np.bool)\n",
        "        \n",
        "        # Aufgabe 3\n",
        "        # post processing of intrinsic rewards\n",
        "        mb_int_rewards = mb_int_rewards\n",
        "        \n",
        "        # choose action then we get (actions, values, log_probs)\n",
        "        last_values = self.model.choose_action(self.obs)[1]\n",
        "        last_int_values = self.model.choose_action(self.obs)[2]\n",
        "\n",
        "        # discount/bootstrap off value fn\n",
        "        mb_returns = np.zeros_like(mb_rewards)\n",
        "        mb_advs = np.zeros_like(mb_rewards)\n",
        "        lastgaelam = 0\n",
        "        \n",
        "        mb_int_returns = np.zeros_like(mb_int_rewards)\n",
        "        mb_int_advs = np.zeros_like(mb_int_rewards)\n",
        "        int_lastgaelam = 0\n",
        "        \n",
        "        for t in reversed(range(self.nsteps)):\n",
        "            \n",
        "            if t == self.nsteps - 1:\n",
        "                nextnonterminal = 1.0 - self.dones\n",
        "                nextvalues = last_values\n",
        "                next_intvalues = last_int_values\n",
        "            else:\n",
        "                nextnonterminal = 1.0 - mb_dones[t+1]\n",
        "                nextvalues = mb_values[t+1]\n",
        "                next_intvalues = mb_int_values[t+1]\n",
        "                \n",
        "            delta = mb_rewards[t] + self.gamma * nextvalues * nextnonterminal - mb_values[t]\n",
        "            mb_advs[t] = lastgaelam = delta + self.gamma * self.lam * nextnonterminal * lastgaelam\n",
        "            \n",
        "            int_delta = mb_int_rewards[t] + self.int_gamma * next_intvalues * nextnonterminal - mb_int_values[t]\n",
        "            mb_int_advs[t] = int_lastgaelam = int_delta + self.int_gamma * self.lam * nextnonterminal * int_lastgaelam\n",
        "        \n",
        "        mb_returns = mb_advs + mb_values\n",
        "        mb_int_returns = mb_int_advs + mb_int_values\n",
        "\n",
        "\n",
        "        return (*map(sf01, (mb_obs, mb_returns, mb_int_rewards, mb_int_returns, mb_dones, mb_actions, mb_values, mb_int_values,  mb_neglogpacs)),\n",
        "            np.sum(mb_rewards, axis = 0))\n",
        "        \n",
        "def sf01(arr):\n",
        "    \"\"\"\n",
        "    swap and then flatten axes 0 and 1\n",
        "    \"\"\"\n",
        "    s = arr.shape\n",
        "    return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc-vyPNr9Mc7"
      },
      "source": [
        "## Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gFhN_7UB9Mc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "28bd27bd-ad15-49c1-c09e-6ee401089a51"
      },
      "source": [
        "ppo = PPO()\n",
        "runner = Runner(env=env, model=ppo, nsteps=TRAJ_LEN, gamma=GAMMA, int_gamma=INT_GAMMA, lam=LAMBDA, rnd_start=RND_START)\n",
        "\n",
        "# model load with check\n",
        "if LOAD and os.path.isfile(PRED_PATH) and os.path.isfile(TARGET_PATH):\n",
        "    ppo.load_model()\n",
        "    pkl_file = open(RESULT_PATH,'rb')\n",
        "    result = pickle.load(pkl_file)\n",
        "    pkl_file.close()\n",
        "    print('Load complete!')\n",
        "else:\n",
        "    result = []\n",
        "    print('Initialize results!')\n",
        "\n",
        "print('Collecting experience...')\n",
        "\n",
        "# episode step for accumulate reward \n",
        "epinfobuf = deque(maxlen=100)\n",
        "# in PPO, we iterate over optimization step\n",
        "nbatch = N_ENVS * TRAJ_LEN\n",
        "# nupdates = N_STEP// nbatch\n",
        "nupdates = 250\n",
        "# check learning time\n",
        "start_time = time.time()\n",
        "\n",
        "for update in tqdm(range(1, nupdates+1)):\n",
        "    # get minibatch\n",
        "    obs, returns, int_rewards, int_returns, masks, actions, values, int_values, neglogpacs, rewards = runner.run()\n",
        "    epinfobuf.append(rewards.sum())\n",
        "    \n",
        "    if ppo.memory_counter > RND_START:\n",
        "        # calculate loss\n",
        "        inds = np.arange(nbatch)\n",
        "        for _ in range(N_OPT_EPOCHS):\n",
        "            a_losses, c_losses, e_losses, t_losses = list(), list(), list(), list()\n",
        "            # shuffle indices for i.i.d.\n",
        "            np.random.shuffle(inds)\n",
        "            # 0 to batch_size with batch_train_size step\n",
        "            for start in range(0, nbatch, BATCH_SIZE):\n",
        "                end = start + BATCH_SIZE\n",
        "                mbinds = inds[start:end]\n",
        "                slices = (arr[mbinds] for arr in (obs, returns, int_returns, masks, actions, values, int_values, neglogpacs))\n",
        "                actor_loss, critic_loss, ent_loss, total_loss = ppo.learn(*slices)\n",
        "                if np.random.rand() <= 0.25:\n",
        "                    ppo.learn_predict(obs[mbinds])\n",
        "            \n",
        "        if update % LOG_FREQ == 0:\n",
        "            # print log and save\n",
        "            # check time interval\n",
        "            time_interval = round(time.time() - start_time, 2)\n",
        "            # calc mean return\n",
        "            mean_100_ep_return = round(np.mean(list(epinfobuf)),2)\n",
        "            result.append(mean_100_ep_return)\n",
        "            # print epi log\n",
        "            print('N : ',update,\n",
        "                  '| Return mean: ', mean_100_ep_return,\n",
        "                  '| R_int_mean : ', round(np.mean(int_rewards),3),\n",
        "                  '| R_int_std : ', round(np.std(int_rewards),3),\n",
        "                  '| Values : ', round(np.mean(values), 3),\n",
        "                  '| Int_values : ', round(np.mean(int_values), 3),\n",
        "                  '| Time:',time_interval,\n",
        "                  '| Used Step:',ppo.memory_counter*N_ENVS)\n",
        "            # save model\n",
        "            if SAVE:\n",
        "                ppo.save_model()\n",
        "env.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/250 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initialize results!\n",
            "Collecting experience...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 1/250 [00:01<08:04,  1.94s/it]\u001b[A\n",
            "  1%|          | 2/250 [00:03<07:54,  1.91s/it]\u001b[A\n",
            "  1%|          | 3/250 [00:06<08:29,  2.06s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c8d1e7f8452c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnupdates\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# get minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mepinfobuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-22fba44a5d40>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# Take actions in env and look the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# Infos contains a ton of useful informations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_counter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnd_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/baselines/baselines/common/vec_env/vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/baselines/baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m#    action = int(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-49227b321170>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-49227b321170>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEpisodicLifeEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-49227b321170>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-49227b321170>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-49227b321170>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFireResetEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfwd3Ynu9MdB"
      },
      "source": [
        "## Plot extrinsic reward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gUnSea-d9MdB"
      },
      "source": [
        "plt.plot(range(len(result)), result)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFieikFFp0sz"
      },
      "source": [
        "## Watch the agent playing!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdoR9p2v9MdC"
      },
      "source": [
        "from matplotlib import animation\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "        \n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
        "    anim.save('rnd_breakout_result.gif', writer='imagemagick', fps=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4_mgM7t9MdC"
      },
      "source": [
        "env = wrap_deepmind(make_atari(ENV_NAME))\n",
        "s = np.array(env.reset())\n",
        "total_reward = 0\n",
        "frames = []\n",
        "done_counter = 0\n",
        "\n",
        "for t in range(10000):\n",
        "    # Render into buffer. \n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    a, v, int_v, l = ppo.choose_action(np.expand_dims(s,axis=0))\n",
        "    # take action and get next state\n",
        "    s_, r, done, info = env.step(a)\n",
        "    s_ = np.array(s_)\n",
        "    total_reward += r\n",
        "    if done:\n",
        "        done_counter += 1\n",
        "        if done_counter == 5:\n",
        "            break\n",
        "    s = s_\n",
        "env.close()\n",
        "print('Total Reward : %.2f'%total_reward)\n",
        "anim = display_frames_as_gif(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRnbnkKXjiZW"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(open('rnd_breakout_result.gif','rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8qqGRHq9MdD"
      },
      "source": [
        "![alt text](./rnd_breakout_result.gif \"segment\")"
      ]
    }
  ]
}